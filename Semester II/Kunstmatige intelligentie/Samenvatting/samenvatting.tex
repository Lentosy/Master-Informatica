\documentclass{report}
\usepackage{ugentstyle}
\usepackage{lipsum}

\begin{document}
\maketitle{Kunstmatige intelligentie}
\tableofcontents
\chapter{Inleiding}
	\begin{itemize}
		\item Twee doelen van kunstmatige intelligentie:
			\begin{itemize}
			\item Het laten overnemen, door machines, van taken waarvoor intelligentie vereist is.
			\item Studie van natuurlijke intelligentie.
			\end{itemize}
		\item Twee vormen om kennis in te brengen in een computersysteem:
			\begin{itemize}
				\item Expliciete kennis.
				\item Kennis kan zelf verworven worden.
			\end{itemize}
	\end{itemize}
\section{Kunnen machines denken?}
\begin{itemize}
	\item Twee voorbeelden.
	\begin{itemize}
		\item ELIZA:
		\begin{itemize}
			\item Computerprogramma dat zich voordoet als een pyschotherapeut.
			\item Maakt gebruik van simpele vervangingsregels.
			\item Probeert de conversatie zo te sturen zodat de echte persoon het meest moet vertellen.
		\end{itemize} 
		\item Chinese kamer:
		\begin{itemize}
			\item Denkrichting die aantoont dat een entiteit eerst iets moet begrijpen, vooraleer er van intelligentie sprake is. 
			\begin{enumerate}
				\item Iemand die geen Chinees kent wordt in een kamer gebracht.
				\item Door een luik krijgt hij briefjes in het Chinees aangereikt, en de bedoeling is dat hij daar schriftelijk een zinnige antwoord op teruggeeft.
				\item De persoon krijgt handboeken waarin conversieregels staan.
			\end{enumerate}
			\item De proefpersoon volgt mechanisch de regels vanuit het handboek, zodat hij wel intelligent gedrag vertoont, maar de berichten niet begrijpt.
		\end{itemize}
	\end{itemize}
	\item \textbf{Denken is elke vorm van complexe informatieverwerking waarvan de onderliggende mechanismen niet volledig gekend zijn.}
	\item \textbf{Turingtest}:
	\begin{itemize}
		\item Proefpersoon kan contact maken met twee entiteiten: een mens en een machine, maar hij weet niet wie de mens of machine is.
		\item De proefpersoon kan eender welke vragen stellen aan beide entiteiten.
		\item Als de proefpersoon er niet in slaagt om na zijn vragenronde de entiteit aan te duiden die een machine is, dan is de machine geslaagd voor de Turingtest.
	\end{itemize} 
\end{itemize}
\section{Toepassingen van AI en data mining}
\begin{itemize}
	\item \textbf{Classificatie:} 
	\begin{itemize}
		\item Stel een verzameling van $k$ klassen.
		\item Een bepaalde invoer met gelinkt worden aan één van die klassen.
		\item \uline{Harde classificatie:} beperkt aantal duidelijk van elkaar gescheiden klassen. Hier spreekt men ook van patroonherkenning.
		\item \uline{Zachte classificatie:} continue overgang van de klassen. 
	\end{itemize}
	\item Toepassingen:
	\begin{itemize}
		\item Aanbevelingssystemen.
		\item Kwaliteitscontrole.
	\end{itemize}
	\item \uline{Probleemgestuurd}: uitgaande van een probleem een oplossing zoeken.
	\item \uline{Datagestuurd}: vanuit bestaande informatie problemen zoeken die ermee opgelost kunnen worden. Dit wordt ook data mining genoemd. Vaak moet de data eerst gereorganiseerd worden vooraleer de informatie nuttig wordt.
\end{itemize}
\section{Leren}
\begin{itemize}
	\item Moderne AI houdt zich bezig met systemen met een zeer groot aantal aanpasbare parameters. Zulke systemen noemt men \textbf{massief lerende systemen}.
	\item \textbf{Voorbeelden} van massief lerende systemen:
	\begin{itemize}
		\item Neurale netwerken: trachten het biologische denksysteem na te bootsen.
		\item Hidden Markov Model: wordt gebruikt bij de analyse van allerhande sequenties, waarbij de toestand soms onbekend is.
	\end{itemize}
	\item Parameters hebben niet noodzakelijk een betekenis, en is daarom ook onmogelijk om ze met de hand in te voeren. Daarom laat men een systeem leren, met behulp van \textbf{drie methoden}:
	\begin{itemize}
		\item Algoritmisch leren: Er wordt gedemonstreerd hoe een bepaalde actie moet uitgevoerd worden. Het systeem kan hierna deze actie inoefenen door het herhalen van deze instructies. Deze vorm komt goed overeen met het programmeren van een computer.
		\item Leren met supervisie: Hier wordt er geen gebruik gemaakt van een algoritme maar eerder van voorbeelden. Deze voorbeelden worden een leerverzameling genoemd en bevatten inputgegevens die het systeem moet leren herkennen, met de daarbij horende resultaten. Er wordt een verband opgelegd tussen een bepaalde input en output.
		\item Leren zonder supervisie: Dit gebeurt gedeeltelijk algoritmisch aangezien er enige instructies nodig zijn om de machine op gang te krijgen. De machine zal nadien zelf experimenteren wat er gebeurd bij het aanpassen van verschillende parameters. Het leren gebeurt dus niet met voorbeelden, maar uit eigen ervaring. Hier is er dan ook geen verband tussen het resultaat en de verschillende deeltaken, maar er is wel een algemeen idee wat er aangeleerd moet worden.
	\end{itemize}
\end{itemize}
\section{Classificatie}
\begin{itemize}
	\item Classificatie is het mappen van een bepaalde input op een klasse.
	\item We spreken van een \textbf{item} dat we moeten klasseren.
	\item Dit item wordt gekarakteriseerd door een aantal \textbf{meetwaarden}.
	\item \underline{Twee soorten meetwaarden}:
	\begin{itemize}
		\item Sommige metingen kunnen een groot aantal waarden opleveren die voorgesteld kunnen worden als een getal.
		\item Andere metingen hebben maar een beperkt aantal waarden, zoals de indeling van van categorieën. 
		\begin{itemize}
			\item Deze kunnen omgezet worden zodat ze antwoorden zijn op ja-nee vragen, zoals het gevolg dat ze geconverteerd kunnen worden naar 0 of 1.
			\good Nu zijn alle meetwaarden getallen.	
		\end{itemize}
	\end{itemize}
	\item Aangezien dat alle meetwaarden getallen zijn $\rightarrow$ standaardvorm: de computer heeft een aantal klassen en moet een getallenrij die de meetwaarden voor een bepaal item bevat toewijzen aan één van de klassen.
	\item Hoe worden de getallenrijen weergegeven? Een aantal notaties:
	\begin{itemize}
		\item De $n$-dimensionale Euclidische ruimte is de de verzameling vectoren met $n$ reële coördinaten.
		\item Zo een vector wordt voorgesteld door een vette letter: $\textbf{v} = (v_1, ..., v_n)$.
		\item Soms vanaf 0 beginnen, zodat we $n+1$-dimensionale vectoren hebben: $\textbf{v} = (v_0, v_1, ..., v_n)$. De waarde $v_0$ krijgt een speciale betekenis.
		\item Reeële getallen die niet deel uitmaken van een vector worden weergegeven met hoofdletters: A, B, ...
		\item De nulvector: 0 = (0, ..., 0).
		\item Het inproduct:
		$$\textbf{v}\cdot\textbf{u} = \sum_{i}^{n}v_iu_i$$
		Het inproduct is lineair: $(A\textbf{v})\cdot \textbf{u} = A(\textbf{v}\cdot\textbf{u})$ en $(\textbf{u} + \textbf{v})\cdot\textbf{x} = \textbf{u} \cdot \textbf{x} + \textbf{v} \cdot \textbf{x}$.
		
		Hieruit volgt de norm $|| \cdot ||$: 
		$$||u||^2 = \textbf{u} \cdot \textbf{u}$$
		\item $d(u,v) = ||\textbf{u} -\textbf{v}||$ is de lengte van de kortste weg van \textbf{u} naar \textbf{v}
		 Hieruit volgt:
		$$||\textbf{u} + \textbf{v}|| \leq ||\textbf{u}||+||\textbf{v}||$$
		 Het kwadraat van beide kanten geeft:
		 $$\textbf{u} \cdot \textbf{v} \leq ||\textbf{u}||\;||\textbf{v}||$$
		 Aangezien dat 
		 $$\cos(\textbf{u}, \textbf{v}) = \frac{\textbf{u} \cdot \textbf{v}}{||\textbf{u} ||\;|| \textbf{v} ||}$$
		 kan dit omgevormd worden tot de volgende ongelijkheid:
		 $$-1 \leq \frac{\textbf{u} \cdot \textbf{v}}{||\textbf{u} ||\;|| \textbf{v} ||} \leq 1$$
		 \item De afstand en de cosinus geven vaak een goede indruk in hoeverre twee vectoren op elkaar lijken. De cosinus geeft een goede maat voor de afstand tussen twee genormaliseerde vectoren:
		 $$d\bigg(\frac{\textbf{u}}{||\textbf{u}||}, \frac{\textbf{v}}{||\textbf{v}||}\bigg)^2 = 2 - 2\cos(\textbf{u}, \textbf{v})$$
	\end{itemize}
\end{itemize}

\section{Informatie en beslissingsbomen}
\begin{itemize}
	\item De \textbf{informatie-inhoud} van een bericht:
	\begin{itemize}
		\item Een bericht is enkel nuttig indien ontvanger een betekenis kan geven aan het bericht. De belangrijke elementen voor de informatie-inhoud is dus het bericht zelf en de kennis van de ontvanger.
		\item Met de kennis kan aan elk mogelijk bericht $B$ een waarschijnlijkheid $P(B)$ toekennen. De informatie-inhoud wordt dan gedefinieerd door
		$$-\log_2(P(B))\; \hbox{bits}$$ 
		Voor $P(B) = 1$ is de informatie-inhoud 0 bits, wat logisch is aangezien de ontvanger niets heeft bijgeleerd van dit bericht.
		\alert De informatie-inhoud van een bericht is niet altijd een geheel getal.
		\alert De informatie-inhoud is nooit negatief.
		\item \underline{Voorbeeld:} Stel dat een byte verwacht wordt, maar er is geen idee welke byte. Elke byte is even waarschijnlijk met kans $1/256$. De informatie-inhoud van de byte die dan binnenkomt is $-\log_2 (1/256)\;\hbox{bits} = 8 \;\hbox{bits}$.
		\item \underline{Voorbeeld:} Stel een alfabet van 4 letters: A, C, G en T. De waarschijnlijkheid dat ze voorkomen wordt weergegeven in tabel \ref{table:example_entropy}.
		\begin{table}[h]
			\centering
			\begin{tabular}{l | r}
				A & 70,71 \% \\
				C & 12,50 \% \\
				G &  8,39 \% \\
				T & 8,39 \% \\
			\end{tabular}
			\caption{De waarschijnlijkheden voor de letters A, C, G en T.}
			\label{table:example_entropy}
		\end{table}
		Als de ontvanger dit weet dan wordt de informatie-inhoud voor elke letter:
		\begin{equation*}
			\begin{split}
				A: -\log_2(0,7071) & = 0,5 \\
				C: -\log_2(0,1250) & = 3,0 \\
				G: -\log_2(0,0839) & = 3,575\\
				T: -\log_2(0,0839) & = 3,575
			\end{split}
		\end{equation*}
	\end{itemize}
	\item Een \textbf{beslissingsboom} is een klassiek hulpmiddel bij classificatie:
	\begin{itemize}
		\item Elke knoop dat geen blad is bevat een vraag met een beperkt mogelijk aantal antwoorden.
		\item Elk mogelijk antwoord verwijst naar een kind van de knoop.
		\item Een item klasseren is een pad vanuit de wortel naar een blad, waarin de klasse staat.
		\item Beslissingsboom opstellen die de ontbrekende informatie oplevert:
		\begin{itemize}
			\item Stel $k$ klassen $K_1, K_2, ... K_k$.
			\item Stel een verzameling $S$ van items waarbij:
			\begin{itemize}
				\item $A(S, i)$ het aantal elementen horend bij $K_i$ is in de verzameling en,
				\item $|S| = \sum_{i = 1}^k A(S,i)$ het totaal aantal element is van $S$.
			\end{itemize} 
			\item De informatie geleverd door een correcte klassering van alle element is dan:
			\begin{equation*}
				\begin{split}
					E(S) & = \sum_{i = 1}^{k} A(S, i) \bigg(-log_2 \bigg( \frac{A(S, i)}{|S|}\bigg) \bigg) \\
					     & = |S|log_2(|S|) +  \sum_{i = 1}^{k} A(S, i)(-log_2(A(S, i)))
				\end{split}
			\end{equation*}
		\end{itemize}
	\end{itemize}
\end{itemize}
\section{Klasseren zonder leren}
\section{Een toepassing: Watson}
\end{document}