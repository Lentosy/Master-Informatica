% Encoding: UTF-8
@article{popperoland2009,
	author  = {Poppe Roland},
	title   = {A survey on vision-based human action recognition},
	year    = {2009}
}

@inproceedings{xia2012view,
	title		= {View invariant human action recognition using histograms of 3D joints},
	author		= {Xia, L. and Chen, C.C. and Aggarwal, JK},
	booktitle	= {Computer Vision and Pattern Recognition Workshops (CVPRW), 2012 IEEE Computer Society Conference on},
	year		= {2012},
	organization= {IEEE}
} 

@Inproceedings {real-time-human-pose-recognition-in-parts-from-a-single-depth-image,
	author = {Shotton, Jamie and Fitzgibbon, Andrew and Blake, Andrew and Kipman, Alex and Finocchio, Mark and Moore, Bob and Sharp, Toby},
	title = {Real-Time Human Pose Recognition in Parts from a Single Depth Image},
	year = {2011},
	month = {June},
	abstract = {
	
	We propose a new method to quickly and accurately predict 3D positions of body joints from a single depth image, using no temporal information. We take an object recognition approach, designing an intermediate body parts representation that maps the difficult pose estimation problem into a simpler per-pixel classification problem. Our large and highly varied training dataset allows the classifier to estimate body parts invariant to pose, body shape, clothing, etc. Finally we generate confidence-scored 3D proposals of several body joints by reprojecting the classification result and finding local modes.
	
	The system runs at 200 frames per second on consumer hardware. Our evaluation shows high accuracy on both synthetic and real test sets, and investigates the effect of several training parameters. We achieve state of the art accuracy in our comparison with related work and demonstrate improved generalization over exact whole-skeleton nearest neighbor matching.
	
	
	},
	publisher = {IEEE},
	url = {https://www.microsoft.com/en-us/research/publication/real-time-human-pose-recognition-in-parts-from-a-single-depth-image/},
	edition = {CVPR},
	note = {Best Paper Award},
}
{
	keep-it-simple-and-sparse-real-time-action-recognition,
	author= {Sean Ryan Fanello, Illaria Gori, Giorgio Metta},
	title = {Keep It Simple And Sparse: Real-Time Action Recognition},
	year = {2013}
}
@article{human-action-recognition-with-extremities-as-semantic-posture-representation,
	author = {Yu, Elden and Aggarwal, J.K.},
	year = {2009},
	month = {06},
	pages = {},
	title = {Human action recognition with extremities as semantic posture representation},
	journal = {2009 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2009},
	doi = {10.1109/CVPRW.2009.5204242}
}
{doi:10.4218/etrij.11.0110.0314,
	author = {Uddin, Md. Zia and Thang, Nguyen Duc and Kim, Jeong Tai and Kim, Tae-Seong},
	title = {Human Activity Recognition Using Body Joint-Angle Features and Hidden Markov Model},
	journal = {ETRI Journal},
	volume = {33},
	number = {4},
	pages = {569-579},
	keywords = {Body-joint-angle features, hidden Markov model (HMM), human activity recognition (HAR)},
	doi = {10.4218/etrij.11.0110.0314},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.4218/etrij.11.0110.0314},
	eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.4218/etrij.11.0110.0314},
	abstract = {This paper presents a novel approach for human activity recognition (HAR) using the joint angles from a 3D model of a human body. Unlike conventional approaches in which the joint angles are computed from inverse kinematic analysis of the optical marker positions captured with multiple cameras, our approach utilizes the body joint angles estimated directly from time-series activity images acquired with a single stereo camera by co-registering a 3D body model to the stereo information. The estimated joint-angle features are then mapped into codewords to generate discrete symbols for a hidden Markov model (HMM) of each activity. With these symbols, each activity is trained through the HMM, and later, all the trained HMMs are used for activity recognition. The performance of our joint-angleâ€“based HAR has been compared to that of a conventional binary and depth silhouette-based HAR, producing significantly better results in the recognition rate, especially for the activities that are not discernible with the conventional approaches.},,
	year = {2011}
}
@Inproceedings {action-recognition-based-bag-3d-points,
	author = {Li, Wanqing and Zhang, Zhengyou and Liu, Zicheng},
	title = {Action Recognition Based on a Bag of 3D Points},
	year = {2010},
	month = {June},
	abstract = {This paper presents a method to recognize human actions from sequences of depth maps. Specifically, we employ an action graph to model explicitly the dynamics of the actions and a bag of 3D points to characterize a set of salient postures that correspond to the nodes in the action graph. In addition, we propose a simple, but effective projection based sampling scheme to sample the bag of 3D points from the depth maps. Experimental results have shown that over 90% recognition accuracy were achieved by sampling only about 1% 3D points from the depth maps. Compared to the 2D silhouette based recognition, the recognition errors were halved. In addition, we demonstrate the potential of the bag of points posture model to deal with occlusions through simulation.},
	publisher = {IEEE},
	url = {https://www.microsoft.com/en-us/research/publication/action-recognition-based-bag-3d-points/},
	pages = {9-14},
	edition = {2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition-Workshops},
}

@article{enhanced-computer-vision-with-microsoft-kinect-sensor,
	author={J. {Han} and L. {Shao} and D. {Xu} and J. {Shotton}},
	journal={IEEE Transactions on Cybernetics},
	title={Enhanced Computer Vision With Microsoft Kinect Sensor: A Review},
	year={2013},
	volume={43},
	number={5},
	pages={1318-1334},
	keywords={computer vision;gesture recognition;image sensors;object recognition;object tracking;Microsoft Kinect sensor;RGB sensing;Kinect-based computer vision algorithm;object tracking;object recognition;human activity analysis;hand gesture analysis;indoor 3D mapping;Computer vision;Data integration;Sensors;Algorithm design and analysis;Cameras;Object recognition;Feature extraction;Computer vision;depth image;information fusion;Kinect sensor;Actigraphy;Actigraphy;Algorithms;Artificial Intelligence;Computer Peripherals;Computer Simulation;Image Enhancement;Image Enhancement;Imaging, Three-Dimensional;Pattern Recognition, Automated;Transducers;Video Games;Whole Body Imaging;Whole Body Imaging},
	doi={10.1109/TCYB.2013.2265378},
	ISSN={2168-2267},
	month={Oct}
}

@conference{Real-time-Human-Motion-Analysis-by-Image-Skeletonization,
	author = {Hironobu Fujiyoshi and Alan Lipton},
	title = {Real-time Human Motion Analysis by Image Skeletonization},
	booktitle = {Proc. of the Workshop on Application of Computer Vision},
	year = {1998},
	month = {October},
} 

@Article{detecting-complex-3d-human-motions-with-body-model-low-rank-representation-foror-real-time-smart-activity-monitoring-system,
  author = {Ahmad Jalal and Shaharyar Kamal and Dong-Seong Kim},
  title  = {Detecting Complex 3D Human Motions with Body Model Low-Rank Representation for Real-Time Smart Activity Monitoring System},
  year   = {2018},
  month  = {March},
}

@Article{Temporal-Action-Detection-with-Structured-Segment-Networks,
  author = {Yue Zhao and Yuanjun Xiong and Limin Wang and Zhirong Wu and Xiaoou Tang and Dahua Lin},
  title  = {Temporal Action Detection with Structured Segment Networks},
  year   = {2017},
}

@article{singh2016untrimmed,
	title={Untrimmed Video Classification for Activity Detection: submission to ActivityNet Challenge},
	author={Singh, Gurkirt and Cuzzolin, Fabio},
	journal={arXiv preprint arXiv:1607.01979},
	year={2016}
}

@inproceedings{Temporal-Action-Localization-with-Pyramid-of-Score-Distribution-Features,
	author = {Yuan, Jun and Ni, Bingbing and Yang, Xiaokang and Kassim, Ashraf},
	year = {2016},
	month = {06},
	pages = {3093-3102},
	title = {Temporal Action Localization with Pyramid of Score Distribution Features},
	doi = {10.1109/CVPR.2016.337}
}

@article{A-survey-of-human-motion-analysis-using-depth-imagery,
	author = {Chen, Lulu and Wei, Hong and Ferryman, James},
	year = {2013},
	month = {11},
	pages = {1995-2006},
	title = {A survey of human motion analysis using depth imagery},
	volume = {34},
	journal = {Pattern Recognition Letters},
	doi = {10.1016/j.patrec.2013.02.006}
}

@article{Action-Recognition-Using-Rate-Invariant-Analysis-of-Skeletal-Shape-Trajectories,
	author = {Ben Amor, Boulbaba},
	year = {2015},
	month = {05},
	pages = {},
	title = {Action Recognition Using Rate-Invariant Analysis of Skeletal Shape Trajectories},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence}
}

@article{Review-of-Action-Recognition-and-Detection-Methods,
	author = {Min Kang, Soo and Wildes, Richard},
	year = {2016},
	month = {10},
	pages = {},
	title = {Review of Action Recognition and Detection Methods}
}

@InProceedings{Modeling-Temporal-Structure-of-Decomposable-Motion-Segments-for-Activity-Classification,
  author = {Niebles, Juan Carlos and Chen, Chih-Wei and Li, Fei Fei},
  title  = {Modeling Temporal Structure of Decomposable Motion Segments for Activity Classification},
  year   = {2010},
  volume = {6312},
  pages  = {392-405},
  month  = {09},
  doi    = {10.1007/978-3-642-15552-9_29},
}

@paper{Self-Adaptive-Proposal-Model-for-Temporal-Action-Detection-Based-on-Reinforcement-Learning,
	author = {Jingjia Huang and Nannan Li and Tao Zhang and Ge Li and Tiejun Huang and Wen Gao},
	title = {SAP: Self-Adaptive Proposal Model for Temporal Action Detection Based on Reinforcement Learning},
	conference = {AAAI Conference on Artificial Intelligence},
	year = {2018},
	keywords = {Computer vision; Action detection; Reinforcement learning},
	url = {https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16109}
}

@InProceedings{DeboeverieCIG2016,
	author      = {Deboeverie, Francis and Roegiers, Sanne and Allebosch, Gianni and Veelaert, Peter and Philips, Wilfried},
	title       = {Human Gesture Classification by Brute-Force Machine Learning for Exergaming in Physiotherapy},
	booktitle   = {2016 IEEE Conference on Computational Intelligence and Games (CIG)},
	year        = {2016},
	pages       = {1-7},
	month       = {Sept},
	affiliation = {iminds},
	doi         = {10.1109/CIG.2016.7860414},
	isbn        = {978-1-5090-1883-3},
	issn        = {2325-4289},
	keywords    = {computer games,gesture recognition,image classification,learning (artificial intelligence),patient treatment,random processes,support vector machines,Microsoft Research Cambridge-12 Kinect dataset,adaptive game development,automatic gesture recognition,brute-force classification strategy,brute-force machine learning,dynamic gesture recognition,exergaming,human gesture classification,leave-one-subject-out cross-validation,online continuous human gesture recognition,physiotherapy,random forests,self-captured stealth game gesture dataset,support vector machines,temporal dimension,Decision trees,Games,Gesture recognition,Radio frequency,Skeleton,Training,Vegetation},
	url         = {http://ieeexplore.ieee.org/document/7860414/},
}

@Article{Action-Recognition-using-Key-Frame-Features-of-Depth-Sequence-and-ELM,
  author  = {Suolan Liu and Hongyuan Wang},
  title   = {Action Recognition using Key-Frame Features of Depth Sequence and ELM},
  journal = {(IJACSA) International Journal of Advanced Computer Science and Applications, Vol. 8, No. 10, 2017},
  year    = {2017},
}

@Article{Action-Recognition-by-Shape-Matching-to-Key-Frames,
  author  = {Carlsson, Stefan and Sullivan, Josephine},
  title   = {Action Recognition by Shape Matching to Key Frames},
  journal = {Workshop on Models versus Exemplars in Computer Vision},
  year    = {2001},
  month   = jan,
}

@Comment{jabref-meta: databaseType:bibtex;}
