Automatically generated by Mendeley Desktop 1.19.3
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Chen2016,
abstract = {This paper presents a human action recognition method by using depth motion maps (DMMs). Each depth frame in a depth video sequence is projected onto three orthogonal Cartesian planes. Under each projection view, the absolute difference between two consecutive projected maps is accumulated through an entire depth video sequence forming a DMM. An l 2 -regularized collaborative represen-tation classifier with a distance-weighted Tikhonov matrix is then employed for action recognition. The developed method is shown to be computationally efficient allowing it to run in real-time. The recognition results applied to the Microsoft Research Action3D dataset indicate superior performance of our method over the existing methods.},
author = {Chen, Chen and Liu, Kui and Kehtarnavaz, Nasser},
doi = {10.1007/s11554-013-0370-1},
file = {:E$\backslash$:/Master-Informatica/Semester II/Masterproef/thesis/bib/Real-time human action recognition based on depth motion maps.pdf:pdf},
issn = {18618200},
journal = {Journal of Real-Time Image Processing},
keywords = {Collaborative representation classifier,Depth motion map,Human action recognition,RGBD camera},
number = {1},
pages = {155--163},
title = {{Real-time human action recognition based on depth motion maps}},
volume = {12},
year = {2016}
}
@article{Han2013,
author = {Han, J and Shao, L and Xu, D and Shotton, J},
doi = {10.1109/TCYB.2013.2265378},
file = {:E$\backslash$:/Master-Informatica/Semester II/Masterproef/thesis/bib/Enhanced Computer Vision with Microsoft Kinect Sensor.pdf:pdf},
issn = {2168-2267},
journal = {IEEE Transactions on Cybernetics},
keywords = {Automated;Transducers;Video Games;Whole Body Imag,Three-Dimensional;Pattern Recognition,computer vision;gesture recognition;image sensors;},
month = {oct},
number = {5},
pages = {1318--1334},
title = {{Enhanced computer vision with microsoft kinect sensor: a review}},
volume = {43},
year = {2013}
}
@inproceedings{Laptev2003,
abstract = {Local image features or interest points provide compact and abstract representations of patterns in an image. We propose to extend the notion of spatial interest points into the spatio-temporal domain and show how the resulting features often reflect interesting events that can be used for a compact representation of video data as well as for its interpretation. To detect spatio-temporal events, we build on the idea of the Harris and Forstner interest point operators and detect local structures in space-time where the image values have significant local variations in both space and time. We then estimate the spatio-temporal extents of the detected events and compute their scale-invariant spatio-temporal descriptors. Using such descriptors, we classify events and construct video representation in terms of labeled space-time points. For the problem of human motion analysis, we illustrate how the proposed method allows for detection of walking people in scenes with occlusions and dynamic backgrounds.},
author = {Laptev and Lindeberg},
booktitle = {Proceedings Ninth IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.2003.1238378},
file = {:E$\backslash$:/Master-Informatica/Semester II/Masterproef/thesis/bib/Space-time Interest Points.pdf:pdf},
isbn = {0-7695-1950-4},
pages = {432--439 vol.1},
publisher = {IEEE},
title = {{Space-time interest points}},
url = {http://ieeexplore.ieee.org/document/1238378/},
year = {2003}
}
@article{Aggarwal2014,
abstract = {Human activity recognition has been an important area of computer vision research since the 1980s. Various approaches have been proposed with a great portion of them addressing this issue via conventional cameras. The past decade has witnessed a rapid development of 3D data acquisition techniques. This paper summarizes the major techniques in human activity recognition from 3D data with a focus on techniques that use depth data. Broad categories of algorithms are identified based upon the use of different features. The pros and cons of the algorithms in each category are analyzed and the possible direction of future research is indicated. {\textcopyright} 2014 Elsevier B.V. All rights reserved.},
author = {Aggarwal, J. K. and Xia, Lu},
doi = {10.1016/j.patrec.2014.04.011},
file = {:E$\backslash$:/Master-Informatica/Semester II/Masterproef/thesis/bib/Human Activity Recognition from 3D Data - A Review.pdf:pdf},
isbn = {9781479956869},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {3D data,Computer vision,Depth image,Human activity recognition},
pages = {70--80},
publisher = {Elsevier B.V.},
title = {{Human activity recognition from 3D data: A review}},
url = {http://dx.doi.org/10.1016/j.patrec.2014.04.011},
volume = {48},
year = {2014}
}
@article{Fathi2007,
abstract = {We present a motion exemplar approach for finding body configuration in monocular videos. A motion correlation technique is employed to measure the motion similarity at various space-time locations between the input video and stored video templates. These observations are used to predict the conditional state, distributions of exemplars and joint positions. Exemplar sequence selection and joint position estimation are then solved with approximate inference using Gibbs sampling and gradient ascent. The presented approach is able to find joint positions accurately for people with textured clothing. Results are presented on a dataset containing slow, fast and incline walk videos of various people from different view angles. The results demonstrate an overall improvement compared to previous methods.},
author = {Fathi, Alireza and Mori, Greg},
doi = {10.1109/ICCV.2007.4409073},
file = {:E$\backslash$:/Master-Informatica/Semester II/Masterproef/thesis/bib/Human Pose Estimation using Motion Exemplars.pdf:pdf},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {1--8},
title = {{Human pose estimation using motion exemplars}},
year = {2007}
}
@article{Shotton2011,
abstract = {We propose a new method to quickly and accurately pre- dict 3D positions ofbody joints from a single depth image, using no temporal information. We take an object recog- nition approach, designing an intermediate body parts rep- resentation that maps the difficult pose estimation problem into a simpler per-pixel classification problem. Our large and highly varied training dataset allows the classifier to estimate body parts invariant to pose, body shape, clothing, etc. Finally we generate confidence-scored 3D proposals of several body joints by reprojecting the classification result and finding local modes. The system runs at 200 frames per second on consumer hardware. Our evaluation shows high accuracy on both synthetic and real test sets, and investigates the effect ofsev- eral training parameters. We achieve state of the art accu- racy in our comparison with related work and demonstrate improved generalization over exact whole-skeleton nearest neighbor matching.},
archivePrefix = {arXiv},
arxivId = {1111.6189v1},
author = {Shotton, Jamie and Fitzgibbon, Andrew and Cook, Mat and Sharp, Toby and Finocchio, Mark},
doi = {10.1007/978-3-642-28661-2-5},
eprint = {1111.6189v1},
file = {:E$\backslash$:/Master-Informatica/Semester II/Masterproef/thesis/bib/Real-Time Human Pose Recognition in Parts from Single Depth Image.pdf:pdf},
isbn = {9783642286605},
issn = {1860949X},
journal = {CVPR},
pmid = {23109523},
title = {{Real-time human pose recognition in parts from single depth images}},
year = {2011}
}
@article{Evenari2014,
author = {Evenari, Michael and Shanan, Leslie and Tadmor, Naphtali},
doi = {10.4159/harvard.9780674419254.c17},
file = {:E$\backslash$:/Master-Informatica/Semester II/Masterproef/thesis/bib/2006-034.pdf:pdf},
isbn = {1-4244-1179-3},
journal = {The Negev},
title = {{XVII. Adaptation of Plants to Desert Conditions. II}},
year = {2014}
}
@article{Kang2016,
author = {Kang, Soo Min and Wildes, Richard},
file = {:E$\backslash$:/Master-Informatica/Semester II/Masterproef/thesis/bib/Review of Action Recognition and Detection Methods.pdf:pdf},
keywords = {Fractional flow reserve,Percutaneous coronary intervention,Stable coronary artery disease},
title = {{Review of Action Recognition and Detection Methods}},
year = {2016}
}
@article{Lv2006,
abstract = {Page 1. Recognition and Segmentation of 3-D Human - ⋆ Fengjun Lv and Ramakant Nevatia University},
author = {Lv, Fengjun and Nevatia, Ramakant},
doi = {10.1007/11744085_28},
file = {:E$\backslash$:/Master-Informatica/Semester II/Masterproef/thesis/bib/Recognition and Segmentation of 3D Human Action Using HMM and Multi-class AdaBoost.pdf:pdf},
isbn = {3540338381},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {359--372},
title = {{Recognition and segmentation of 3-D human action using HMM and multi-class AdaBoost}},
volume = {3954 LNCS},
year = {2006}
}
@article{Hoai2015,
author = {Hoai, Minh and Zisserman, Andrew},
doi = {10.1007/978-3-319-16814-2_1},
file = {:E$\backslash$:/Master-Informatica/Semester II/Masterproef/thesis/bib/Improving Human Action Recognition using Score Distribution and Ranking.pdf:pdf},
isbn = {9783319168135},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {3--20},
title = {{Improving human action recognition using score distribution and ranking}},
volume = {9007},
year = {2015}
}
@article{Baust2010,
abstract = {We introduce a segmentation framework which combines and shares advantages of both an implicit surface representation and a parametric shape model based on spherical harmonics. Besides the elegant surface representation it also inherits the power and flexibility of variational level set methods with respect to the modeling of data terms. At the same time it provides all advantages of parametric shape models such as a sparse and multiscale shape representation. Additionally, we introduce a regularizer that helps to ensure a unique decomposition into spherical harmonics and thus the comparability of parameter values of multiple segmentations. We demonstrate the benefits of our method on medical and photometric data and present two possible extensions.},
archivePrefix = {arXiv},
arxivId = {9780201398298},
author = {Niebles, Juan Carlos and Chen, Chih-Wei and Fei-Fei, Li},
doi = {10.1007/978-3-642-15558-1},
eprint = {9780201398298},
file = {:E$\backslash$:/Master-Informatica/Semester II/Masterproef/thesis/bib/Modeling{\_}Temporal{\_}Structure{\_}of{\_}Decomposable{\_}Motion.pdf:pdf},
isbn = {978-3-642-15557-4},
issn = {03029743},
number = {February 2014},
pages = {580--593},
pmid = {4520227},
title = {{Modeling Temporal Structure of Decomposable Motion Segments for Activity Classification}},
url = {http://www.springerlink.com/content/4x63084761466g3j/},
volume = {6313},
year = {2010}
}
@inproceedings{Yang2012,
abstract = {In this paper, we propose an effective method to recognize human actions from 3D positions of body joints. With the release of RGBD sensors and associated SDK, human body joints can be extracted in real time with reasonable accuracy. In our method, we propose a new type of features based on position differences of joints, EigenJoints, which combine action information including static posture, motion, and offset. We further employ the Nai{\&}{\#}x0308;ve-Bayes-Nearest-Neighbor (NBNN) classifier for multi-class action classification. The recognition results on the Microsoft Research (MSR) Action3D dataset demonstrate that our approach significantly outperforms the state-of-the-art methods. In addition, we investigate how many frames are necessary for our method to recognize actions on the MSR Action3D dataset. We observe 15-20 frames are sufficient to achieve comparable results to that using the entire video sequences.},
author = {Yang, Xiaodong and Tian, Ying Li},
booktitle = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
doi = {10.1109/CVPRW.2012.6239232},
file = {:E$\backslash$:/Master-Informatica/Semester II/Masterproef/thesis/bib/EigenJoints-based Action Recognition Using Naive-Bayes-Nearest-Neighbor.pdf:pdf},
isbn = {9781467316118},
issn = {21607508},
title = {{EigenJoints-based action recognition using Na{\"{i}}ve-Bayes-Nearest- Neighbor}},
year = {2012}
}
@article{Wang2012a,
author = {Wang, Jiang and Liu, Zicheng and Chorowski, Jan and Chen, Zhuoyuan and Wu, Ying},
file = {:E$\backslash$:/Master-Informatica/Semester II/Masterproef/thesis/bib/Robust 3D Action Recognition with Random Occupancy Patterns.pdf:pdf},
title = {{Robust 3d action recognition with random occupancy patterns}},
year = {2012}
}
@article{Vezzani2010,
author = {Vezzani, Roberto and Baltieri, Davide and Cucchiara, Rita},
file = {:E$\backslash$:/Master-Informatica/Semester II/Masterproef/thesis/bib/HMM Based Action Recognition with Projection Histogram Features.pdf:pdf},
title = {{HMM Based Action Recognition with Projection Histogram Features}},
year = {2010}
}
@article{Dollar2005,
archivePrefix = {arXiv},
arxivId = {1705.01861},
author = {Dollar, Piotr and Rabaud, Vincent and Cottrell, Garrison and Belongie, Serge},
doi = {10.3390/jcm7030052},
eprint = {1705.01861},
file = {:E$\backslash$:/Master-Informatica/Semester II/Masterproef/thesis/bib/Behaviour Recognition via Sparse Spatio-Temporal Features.pdf:pdf},
isbn = {0-7803-9424-0},
issn = {2077-0383},
pmid = {17994073},
title = {{Behaviour Recognition via Sparse Spatio-Temporal Features}},
year = {2005}
}
@article{Gowayyed2013,
abstract = {Creating descriptors for trajectories has many applications in robotics/human motion analysis and video copy detection. Here, we propose a novel descriptor for 2D trajectories: Histogram of Oriented Displacements (HOD). Each displacement in the trajectory votes with its length in a histogram of orientation angles. 3D trajectories are described by the HOD of their three projections. We use HOD to describe the 3D trajectories of body joints to recognize human actions, which is a challenging machine vision task, with applications in human-robot/machine interaction, interactive entertainment, multimedia information retrieval, and surveillance. The descriptor is fixed-length, scale-invariant and speed-invariant. Experiments on MSR-Action3D and HDM05 datasets show that the descriptor outperforms the state-of-the-art when using off-the-shelf classification tools.},
author = {Gowayyed, Mohammad A. and Torki, Marwan and Hussein, Mohamed E. and El-Saban, Motaz},
file = {:E$\backslash$:/Master-Informatica/Semester II/Masterproef/thesis/bib/IJCAI13MGowayyed.pdf:pdf},
isbn = {9781577356332},
issn = {10450823},
journal = {IJCAI International Joint Conference on Artificial Intelligence},
keywords = {Machine Learning},
number = {August},
pages = {1351--1357},
title = {{Histogram of Oriented Displacements (HOD): Describing trajectories of human joints for action recognition}},
year = {2013}
}
@article{Han2010,
abstract = {In this paper, we propose a hierarchical discriminative approach for human action recognition. It consists of feature extraction with mutual motion pattern analysis and discriminative action modeling in the hierarchical manifold space. Hierarchical Gaussian Process Latent Variable Model (HGPLVM) is employed to learn the hierarchical manifold space in which motion patterns are extracted. A cascade CRF is also presented to estimate the motion patterns in the corresponding manifold subspace, and the trained SVM classifier predicts the action label for the current observation. Using motion capture data, we test our method and evaluate how body parts make effect on human action recognition. The results on our test set of synthetic images are also presented to demonstrate the robustness. {\textcopyright} 2009 Elsevier B.V. All rights reserved.},
author = {Han, Lei and Wu, Xinxiao and Liang, Wei and Hou, Guangming and Jia, Yunde},
doi = {10.1016/j.imavis.2009.08.003},
issn = {02628856},
journal = {Image and Vision Computing},
keywords = {Discriminative model,Hierarchical manifold learning,Human action recognition,Motion pattern,Mutual invariant},
number = {5},
pages = {836--849},
title = {{Discriminative human action recognition in the learned hierarchical manifold space}},
volume = {28},
year = {2010}
}
@article{Shan2014,
author = {Shan, Junjie and Akella, Srinivas},
doi = {10.1109/ARSO.2014.7020983},
file = {:E$\backslash$:/Master-Informatica/Semester II/Masterproef/thesis/bib/3D Human Action Segmentation and Recognition using Pose Kinetic Energy.pdf:pdf},
isbn = {9781479969685},
journal = {2014 IEEE International Workshop on Advanced Robotics and its Social Impacts},
keywords = {Human-Robot Interaction,Robots in Daily Life},
pages = {69--75},
title = {{3D Human Action Segmentation and Recognition using Pose Kinetic Energy}},
year = {2014}
}
@inproceedings{Weinland2008,
abstract = {In this paper, we address the problem of representing human actions using visual cues for the purpose of learning and recognition. Traditional approaches model actions as space-time representations which explicitly or implicitly encode the dynamics of an action through temporal dependencies. In contrast, we propose a new compact and efficient representation which does not account for such dependencies. Instead, motion sequences are represented with respect to a set of discriminative static key-pose exemplars and without modeling any temporal ordering. The interest is a time-invariant representation that drastically simplifies learning and recognition by removing time related information such as speed or length of an action. The proposed representation is equivalent to embedding actions into a space defined by distances to key-pose exemplars. We show how to build such embedding spaces of low dimension by identifying a vocabulary of highly discriminative exemplars using a forward selection. To test our representation, we have used a publicly available dataset which demonstrates that our method can precisely recognize actions, even with cluttered and non-segmented sequences.},
author = {Weinland, Daniel and Boyer, Edmond},
booktitle = {26th IEEE Conference on Computer Vision and Pattern Recognition, CVPR},
doi = {10.1109/CVPR.2008.4587731},
file = {:E$\backslash$:/Master-Informatica/Semester II/Masterproef/thesis/bib/Action Recognition Using Exemplar based Embedding.pdf:pdf},
isbn = {9781424422432},
title = {{Action recognition using exemplar-based embedding}},
year = {2008}
}
@article{Zhang2016,
abstract = {Human action recognition from RGB-D (Red, Green, Blue and Depth) data has attracted increasing attention since the first work reported in 2010. Over this period, many benchmark datasets have been created to facilitate the development and evaluation of new algorithms. This raises the question of which dataset to select and how to use it in providing a fair and objective comparative evaluation against state-of-the-art methods. To address this issue, this paper provides a comprehensive review of the most commonly used action recognition related RGB-D video datasets, including 27 single-view datasets, 10 multi-view datasets, and 7 multi-person datasets. The detailed information and analysis of these datasets is a useful resource in guiding insightful selection of datasets for future research. In addition, the issues with current algorithm evaluation vis-{\'{a}}-vis limitations of the available datasets and evaluation protocols are also highlighted; resulting in a number of recommendations for collection of new datasets and use of evaluation protocols.},
archivePrefix = {arXiv},
arxivId = {arXiv:1601.05511v1},
author = {Zhang, Jing and Li, Wanqing and Ogunbona, Philip O. and Wang, Pichao and Tang, Chang},
doi = {10.1016/j.patcog.2016.05.019},
eprint = {arXiv:1601.05511v1},
file = {:E$\backslash$:/Master-Informatica/Semester II/Masterproef/thesis/bib/SurveyJournal{\_}r1.pdf:pdf},
isbn = {1878-4216(Electronic),0278-5846(Print)},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Action recognition,Evaluation protocol,RGB-D dataset},
number = {January},
pages = {86--105},
pmid = {19666074},
title = {{RGB-D-based action recognition datasets: A survey}},
volume = {60},
year = {2016}
}
@article{Andrew2002,
abstract = {We compare discriminative and generative learning as typified by logistic regression and naive Bayes. We show, contrary to a widely- held belief that discriminative classifiers are almost always to be preferred, that there can often be two distinct regimes of per- formance as the training set size is increased, one in which each algorithm does better. This stems from the observation- which is borne out in repeated experiments- that while discriminative learning has lower asymptotic error, a generative classifier may also approach its (higher) asymptotic error much faster.},
archivePrefix = {arXiv},
arxivId = {http://dx.doi.org/10.1007/s11063-008-9088-7},
author = {Ng, Andrew and Jordan, Michael},
doi = {10.1007/s11063-008-9088-7},
eprint = {/dx.doi.org/10.1007/s11063-008-9088-7},
file = {:E$\backslash$:/Master-Informatica/Semester II/Masterproef/thesis/bib/On Discriminative vs Generative classifiers - A comparison of logistic regression and naive Bayes.pdf:pdf},
isbn = {1106300890},
issn = {13704621},
keywords = {Asymptotic relative efficiency,Discriminative classifiers,Generative classifiers,Logistic regression,Na{\"{i}}ve Bayes classifier,Normal-based discriminant analysis},
pmid = {25246403},
primaryClass = {http:},
title = {{On discriminative vs. generative classifiers: A comparison of logistic regression and naive bayes}},
year = {2002}
}
@paper{Huang2018,
author = {Huang, Jingjia and Li, Nannan and Zhang, Tao and Li, Ge and Huang, Tiejun and Gao, Wen},
file = {:E$\backslash$:/Master-Informatica/Semester II/Masterproef/thesis/bib/A Self-Adaptive Proposal Model for Temporal Action Detection based on Reinforcement Learning.pdf:pdf},
keywords = {Computer vision; Action detection; Reinforcement l},
title = {{SAP: Self-Adaptive Proposal Model for Temporal Action Detection Based on Reinforcement Learning}},
url = {https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16109},
year = {2018}
}
@article{Elden2009,
author = {Yu, Elden and Aggarwal, J K},
doi = {10.1109/CVPRW.2009.5204242},
file = {:E$\backslash$:/Master-Informatica/Semester II/Masterproef/thesis/bib/Human Action Recognition with Extremities as Semantic Posture Representation.pdf:pdf},
journal = {2009 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2009},
title = {{Human action recognition with extremities as semantic posture representation}},
year = {2009}
}
@article{Fanello2017,
abstract = {Sparsity has been showed to be one of the most important properties for visual recognition purposes. In this paper we show that sparse representation plays a fundamental role in achieving one-shot learning and real-time recognition of actions. We start off from RGBD images, combine motion and appearance cues and extract state-of-the-art features in a computationally efficient way. The proposed method relies on descriptors based on 3D Histograms of Scene Flow (3DHOFs) and Global Histograms of Oriented Gradient (GHOGs); adaptive sparse coding is applied to capture high-level patterns from data. We then propose a simultaneous on-line video segmentation and recognition of actions using linear SVMs. The main contribution of the paper is an effective real-time system for one-shot action modeling and recognition; the paper highlights the effectiveness of sparse coding techniques to represent 3D actions. We obtain very good results on three different data sets: a benchmark data set for one-shot action learning (the ChaLearn Gesture Data Set), an in-house data set acquired by a Kinect sensor including complex actions and gestures differing by small details, and a data set created for human-robot interaction purposes. Finally we demonstrate that our system is effective also in a human-robot interaction setting and propose a memory game, " All Gestures You Can " , to be played against a humanoid robot.},
author = {Fanello, Sean Ryan and Gori, Ilaria and Metta, Giorgio and Odone, Francesca},
doi = {10.1007/978-3-319-57021-1_10},
file = {:E$\backslash$:/Master-Informatica/Semester II/Masterproef/thesis/bib/Keep it Simple And Sparse$\backslash$; Real-Time Action Recognition.pdf:pdf},
keywords = {human,one-shot action learning,real-time action recognition,sparse representation},
pages = {303--328},
title = {{Keep It Simple and Sparse: Real-Time Action Recognition}},
volume = {14},
year = {2017}
}
@article{Zhu2013,
abstract = {We present a novel approach to 3D human action recognition based on a feature-level fusion of spatiotemporal features and skeleton joints. First, 3D interest points detection and local feature description are performed to extract spatiotemporal motion information. Then the frame difference and pairwise distances of skeleton joint positions are computed to characterize the spatial information of the joints in 3D space. These two features are complementary to each other. A fusion scheme is then proposed to combine them effectively based on the random forests method. The proposed approach is validated on three challenging 3D action datasets for human action recognition. Experimental results show that the proposed approach outperforms the state-of-the-art methods on all three datasets. View full abstract},
author = {Zhu, Yu and Chen, Wenbin and Guo, Guodong},
doi = {10.1109/CVPRW.2013.78},
file = {:E$\backslash$:/Master-Informatica/Semester II/Masterproef/thesis/bib/Fusing Spatiotemporal Features and Joints for 3D Action Recognition.pdf:pdf},
isbn = {9780769549903},
issn = {21607508},
journal = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
keywords = {3D action recognition,Human action recognition,fusion},
pages = {486--491},
title = {{Fusing spatiotemporal features and joints for 3D action recognition}},
year = {2013}
}
@inproceedings{Wang2013a,
author = {Wang, Heng and Schmid, Cordelia},
booktitle = {2013 IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.2013.441},
file = {:E$\backslash$:/Master-Informatica/Semester II/Masterproef/thesis/bib/wang{\_}iccv13.pdf:pdf},
isbn = {978-1-4799-2840-8},
month = {dec},
pages = {3551--3558},
publisher = {IEEE},
title = {{Action Recognition with Improved Trajectories}},
url = {http://ieeexplore.ieee.org/document/6751553/},
year = {2013}
}
@phdthesis{Luo2014,
author = {Luo, Jiajia},
doi = {10.4161/auto.26678},
file = {:E$\backslash$:/Master-Informatica/Semester II/Masterproef/thesis/bib/Feature Extraction and Recognition for Human Action Recognition.pdf:pdf},
issn = {1554-8627},
school = {University of Tennessee, Knoxville},
title = {{Feature Extraction and Recognition for Human Action Recognition}},
year = {2014}
}
@article{Willems2008,
author = {Willems, Geert and Tuytelaars, Tinne and Gool, Luc Van},
file = {:E$\backslash$:/Master-Informatica/Semester II/Masterproef/thesis/bib/An Efficient Dense and Scale-Invariant Spatio-Temporal Interest Point Detector.pdf:pdf},
title = {{An efficient dense and scale-invariant spatio-temporal interest point detector}},
year = {2008}
}
@article{Wang2014,
abstract = {Existing methods on video-based action recognition are generally view-dependent, i.e., performing recognition from the same views seen in the training data. We present a novel multiview spatio-temporal AND-OR graph (MST- AOG) representation for cross-view action recognition, i.e., the recognition is performed on the video from an unknown and unseen view. As a compositional model, MST-AOG compactly represents the hierarchical combinatorial struc- tures ofcross-view actions by explicitly modeling the geom- etry, appearance and motion variations. This paper pro- poses effective methods to learn the structure and param- eters ofMST-AOG. The inference based on MST-AOG en- ables action recognition from novel views. The training of MST-AOG takes advantage of the 3D human skeleton data obtained from Kinect cameras to avoid annotating enor- mous multi-view video frames, which is error-prone and time-consuming, but the recognition does not need 3D in- formation and is based on 2D video input. A new Multiview Action3D dataset has been created andwill be released. Ex- tensive experiments have demonstrated that this new action representation significantly improves the accuracy and ro- bustness for cross-view action recognition on 2D videos.},
author = {Wang, Jiang and Nie, Xiaohan and Xia, Yin and Wu, Ying and Zhu, Song-Chun},
file = {:E$\backslash$:/Master-Informatica/Semester II/Masterproef/thesis/bib/Cross-view Action Modeling, Learning and Recognition.pdf:pdf},
title = {{Cross-view Action Modeling , Learning and Recognition}},
year = {2014}
}
@article{Wang2013,
author = {Wang, Jiang and Wu, Ying and Rd, Sheridan and Il, Evanston},
doi = {10.1109/ICCV.2013.334},
file = {:E$\backslash$:/Master-Informatica/Semester II/Masterproef/thesis/bib/Wang{\_}Learning{\_}Maximum{\_}Margin{\_}2013{\_}ICCV{\_}paper.pdf:pdf},
title = {{Learning Maximum Margin Temporal Warping for Action Recognition}},
year = {2013}
}
@article{AngelesMendoza2007,
abstract = {This paper describes an experimental study about a robust contour feature (shape-context) for using in action recognition based on continuous hidden Markov models (HMM). We ran different experimental setting using the KTH's database of actions. The image contours are extracted using a standard algorithm. The shape-context feature vector is build from of histogram of a set ofnon-overlapping regions in the image. We show that the combined use of HMM and this feature gives equivalent o better results, in term of action detection, that current approaches in the literature.},
author = {{{\'{A}}ngeles Mendoza}, M. and {P{\'{e}}rez de la Blanca}, Nicol{\'{a}}s},
doi = {10.1007/978-3-540-72847-4_51},
file = {:E$\backslash$:/Master-Informatica/Semester II/Masterproef/thesis/bib/HMM Based Action Recognition Using Contour Histograms.pdf:pdf},
journal = {Pattern Recognition and Image Analysis},
pages = {394--401},
title = {{HMM-Based Action Recognition Using Contour Histograms}},
year = {2007}
}
@article{Su2014,
abstract = {We consider the statistical analysis of trajectories on Riemannian manifolds that are observed under arbitrary temporal evolutions. Past methods rely on cross-sectional analysis, with the given temporal registration, and consequently may lose the mean structure and artificially inflate observed variances. We introduce a quantity that provides both a cost function for temporal registration and a proper distance for comparison of trajectories. This distance is used to define statistical summaries, such as sample means and covariances, of synchronized trajectories and “Gaussian-type” models to capture their variability at discrete times. It is invariant to identical time-warpings (or temporal reparameterizations) of trajectories. This is based on a novel mathematical representation of trajectories, termed transported square-root vector field (TSRVF), and the L2L2 norm on the space of TSRVFs. We illustrate this framework using three representative manifolds—S2S2, SE(2)SE(2) and shape space of planar contours—involving both simulated and real data. In particular, we demonstrate: (1) improvements in mean structures and significant reductions in cross-sectional variances using real data sets, (2) statistical modeling for capturing variability in aligned trajectories, and (3) evaluating random trajectories under these models. Experimental results concern bird migration, hurricane tracking and video surveillance.},
author = {Su, Jingyong and Kurtek, Sebastian and Klassen, Eric and Srivastava, Anuj},
doi = {10.1214/13-AOAS701},
file = {:E$\backslash$:/Master-Informatica/Semester II/Masterproef/thesis/bib/euclid.aoas.1396966297.pdf:pdf},
issn = {1932-6157},
journal = {The Annals of Applied Statistics},
keywords = {Parallel transport,Rate invariant,Riemannian manifold,Temporal trajectory,Time warping,Variance reduction},
month = {mar},
number = {1},
pages = {530--552},
title = {{Statistical analysis of trajectories on Riemannian manifolds: Bird migration, hurricane tracking and video surveillance}},
url = {http://projecteuclid.org/euclid.aoas/1396966297},
volume = {8},
year = {2014}
}
@inproceedings{Wei2013,
author = {Wei, Ping and Zheng, Nanning and Zhao, Yibiao and Zhu, Song-chun},
booktitle = {2013 IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.2013.389},
file = {:E$\backslash$:/Master-Informatica/Semester II/Masterproef/thesis/bib/Conf{\_}2013{\_}ICCV{\_}ConcurrentAction.pdf:pdf},
isbn = {978-1-4799-2840-8},
month = {dec},
number = {1},
pages = {3136--3143},
publisher = {IEEE},
title = {{Concurrent Action Detection with Structural Prediction}},
url = {http://ieeexplore.ieee.org/document/6751501/},
year = {2013}
}
@article{Wang2014,
author = {Wang, Limin and Qiao, Yu and Tang, Xiaoou},
file = {:E$\backslash$:/Master-Informatica/Semester II/Masterproef/thesis/bib/Action Recognition and Detection by Combining Motion and Appearance Features.pdf:pdf},
title = {{Action recognition and detection by combining motion and appearance features}},
year = {2014}
}
@article{Slama2015,
abstract = {In this paper we address the problem of modeling and analyzing human motion by focusing on 3D body skeletons. Particularly, our intent is to represent skeletal motion in a geometric and efficient way, leading to an accurate action-recognition system. Here an action is represented by a dynamical system whose observability matrix is characterized as an element of a Grassmann manifold. To formulate our learning algorithm, we propose two distinct ideas: (1) in the first one we perform classification using a Truncated Wrapped Gaussian model, one for each class in its own tangent space. (2) In the second one we propose a novel learning algorithm that uses a vector representation formed by concatenating local coordinates in tangent spaces associated with different classes and training a linear SVM. We evaluate our approaches on three public 3D action datasets: MSR-action 3D, UT-kinect and UCF-kinect datasets; these datasets represent different kinds of challenges and together help provide an exhaustive evaluation. The results show that our approaches either match or exceed state-of-the-art performance reaching 91.21{\%} on MSR-action 3D, 97.91{\%} on UCF-kinect, and 88.5{\%} on UT-kinect. Finally, we evaluate the latency, i.e. the ability to recognize an action before its termination, of our approach and demonstrate improvements relative to other published approaches.},
author = {Slama, Rim and Wannous, Hazem and Daoudi, Mohamed and Srivastava, Anuj},
doi = {10.1016/j.patcog.2014.08.011},
file = {:E$\backslash$:/Master-Informatica/Semester II/Masterproef/thesis/bib/Accurate 3D action recognition using learning on the Grassmann manifold.pdf:pdf},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Classification,Depth images,Grassmann manifold,Human action recognition,Observational latency,Skeleton},
number = {2},
pages = {556--567},
publisher = {Elsevier},
title = {{Accurate 3D action recognition using learning on the Grassmann manifold}},
url = {http://dx.doi.org/10.1016/j.patcog.2014.08.011},
volume = {48},
year = {2015}
}
@article{Hussein2013,
author = {Hussein, Mohamed E and Torki, Marwan and Gowayyed, Mohammad A and El-saban, Motaz},
file = {:E$\backslash$:/Master-Informatica/Semester II/Masterproef/thesis/bib/Human Action Recognition Using a Temporal Hierarchy of Covariance Descriptors on 3D Joint Locations.pdf:pdf},
journal = {IJCAI International Joint Conference on Artificial Intelligence},
title = {{Human Action Recognition Using a Temporal Hierarchy of Covariance Descriptors on 3D Joint Locations}},
year = {2013}
}
@article{Carlsson2001,
author = {Carlsson, Stefan and Sullivan, Josephine},
file = {:E$\backslash$:/Master-Informatica/Semester II/Masterproef/thesis/bib/Action Recognition by Shape Matching to Key Frames.pdf:pdf},
journal = {Workshop on Models versus Exemplars in Computer Vision},
month = {jan},
title = {{Action recognition by shape matching to key frames}},
year = {2001}
}
@article{Xia2012,
author = {Xia, Lu and Chen, Chia-chih and Aggarwal, JK},
doi = {10.1109/CVPRW.2012.6239233},
file = {:E$\backslash$:/Master-Informatica/Semester II/Masterproef/thesis/bib/View invariant human action recognition using histograms of 3D joints.pdf:pdf},
isbn = {9781467316125},
journal = {CVPR 2012 HAU3D Workshop},
pages = {20--27},
title = {{View Invariant Human Action Recognition Using Histograms of 3D Joints}},
year = {2012}
}
@article{Dtw,
author = {Dtw, Classical},
file = {:E$\backslash$:/Master-Informatica/Semester II/Masterproef/thesis/bib/Dynamic Time Warping.pdf:pdf},
title = {{4 Dynamic Time Warping}}
}
@article{Tuzel2006,
author = {Tuzel, O and Porikli, F and Vision, P Meer},
file = {:E$\backslash$:/Master-Informatica/Semester II/Masterproef/thesis/bib/TR2005-111.pdf:pdf},
journal = {European conference on computer},
number = {2006},
title = {{Zusammenfassung: Region Covariance: A Fast Descriptor for Detection and Classification (2006)}},
url = {https://link.springer.com/chapter/10.1007/11744047{\_}45},
year = {2006}
}
@article{Wang2012b,
author = {Wang, Jiang and Liu, Zicheng and Wu, Ying and Yuan, Junsong},
file = {:E$\backslash$:/Master-Informatica/Semester II/Masterproef/thesis/bib/Mining Actionlet Ensemble for Action Recognition with Depth Cameras.pdf:pdf},
title = {{Mining Actionlet Ensemble for Action Recognition with Depth Cameras}},
year = {2012}
}
@article{Chen2017,
author = {Zhang, Baochang and Yang, Yun and Chen, Chen},
file = {:E$\backslash$:/Master-Informatica/Semester II/Masterproef/thesis/bib/Action Recognition Using 3D Histograms of Texture and A Multi-Class Boosting Classifier.pdf:pdf},
journal = {IEEE Transactions on Image Processing},
title = {{Action recognition using 3d histograms of texture and a multi-class boosting classifier}},
year = {2017}
}
@inproceedings{Ajili2017,
author = {Ajili, Insaf and Mallem, Malik and Didier, Jean-yves},
booktitle = {2017 26th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN)},
doi = {10.1109/ROMAN.2017.8172443},
file = {:E$\backslash$:/Master-Informatica/Semester II/Masterproef/thesis/bib/RoMan17-prePrint.pdf:pdf},
isbn = {978-1-5386-3518-6},
month = {aug},
pages = {1115--1120},
publisher = {IEEE},
title = {{Gesture recognition for humanoid robot teleoperation}},
url = {http://ieeexplore.ieee.org/document/8172443/},
year = {2017}
}
@inproceedings{Ofli2012,
abstract = {Much of the existing work on action recognition combines simple features with complex classifiers or models to represent an action. Parameters of such models usually do not have any physical meaning nor do they provide any qualitative insight relating the action to the actual motion of the body or its parts. In this paper, we propose a new representation of human actions called sequence of the most informative joints (SMIJ), which is extremely easy to interpret. At each time instant, we automatically select a few skeletal joints that are deemed to be the most informative for performing the current action based on highly interpretable measures such as the mean or variance of joint angle trajectories. We then represent the action as a sequence of these most informative joints. Experiments on multiple databases show that the SMIJ representation is discriminative for human action recognition and performs better than several state-of-the-art algorithms. {\textcopyright} 2013 Elsevier Inc. All rights reserved.},
author = {Ofli, Ferda and Chaudhry, Rizwan and Kurillo, Gregorij and Vidal, Rene and Bajcsy, Ruzena},
booktitle = {2012 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
doi = {10.1109/CVPRW.2012.6239231},
file = {:E$\backslash$:/Master-Informatica/Semester II/Masterproef/thesis/bib/Sequence of the Most Informative Joints (SMIJ) - A New Representation for Human Skeletal Action Recognition.pdf:pdf},
isbn = {978-1-4673-1612-5},
issn = {10473203},
keywords = {Bag-of-words,Berkeley MHAD,Cross-database generalization,HDM05,Human action recognition,Human action representation,Informative joints,Linear dynamical systems,MSR Action3D,Normalized edit distance},
month = {jun},
number = {1},
pages = {8--13},
publisher = {IEEE},
title = {{Sequence of the Most Informative Joints (SMIJ): A new representation for human skeletal action recognition}},
url = {http://ieeexplore.ieee.org/document/6239231/},
volume = {25},
year = {2012}
}
@article{Suolan2017,
author = {Liu, Suolan and Wang, Hongyuan},
file = {:E$\backslash$:/Master-Informatica/Semester II/Masterproef/thesis/bib/Action Recognition Using Key-Frame Features of Depth Sequence and ELM.pdf:pdf},
journal = {(IJACSA) International Journal of Advanced Computer Science and Applications, Vol. 8, No. 10, 2017},
title = {{Action recognition using key-frame features of depth sequence and ELM}},
year = {2017}
}
@article{Huang2017,
abstract = {Existing action detection algorithms usually generate action proposals through an extensive search over the video at multiple temporal scales, which brings about huge computational overhead and deviates from the human perception procedure. We argue that the process of detecting actions should be naturally one of observation and refinement: observe the current window and refine the span of attended window to cover true action regions. In this paper, we propose an active action proposal model that learns to find actions through continuously adjusting the temporal bounds in a self-adaptive way. The whole process can be deemed as an agent, which is firstly placed at a position in the video at random, adopts a sequence of transformations on the current attended region to discover actions according to a learned policy. We utilize reinforcement learning, especially the Deep Q-learning algorithm to learn the agent's decision policy. In addition, we use temporal pooling operation to extract more effective feature representation for the long temporal window, and design a regression network to adjust the position offsets between predicted results and the ground truth. Experiment results on THUMOS 2014 validate the effectiveness of the proposed approach, which can achieve competitive performance with current action detection algorithms via much fewer proposals.},
archivePrefix = {arXiv},
arxivId = {1706.07251},
author = {Huang, Jingjia and Li, Nannan and Zhang, Tao and Li, Ge},
eprint = {1706.07251},
file = {:E$\backslash$:/Master-Informatica/Semester II/Masterproef/thesis/bib/Self-Adaptive Proposal Model for Temporal Action Detection based on Reinforcement Learning.pdf:pdf},
title = {{A Self-Adaptive Proposal Model for Temporal Action Detection based on Reinforcement Learning}},
url = {http://arxiv.org/abs/1706.07251},
year = {2017}
}
@inproceedings{Dalal2010,
author = {Dalal, Navneet and Triggs, Bill},
booktitle = {2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)},
doi = {10.1109/CVPR.2005.177},
file = {:E$\backslash$:/Master-Informatica/Semester II/Masterproef/thesis/bib/hog{\_}cvpr2005.pdf:pdf},
isbn = {0-7695-2372-2},
pages = {886--893},
publisher = {IEEE},
title = {{Histograms of Oriented Gradients for Human Detection}},
url = {http://ieeexplore.ieee.org/document/1467360/},
volume = {1},
year = {2015}
}
@article{Mnih2012,
abstract = {Conditional Restricted Boltzmann Machines (CRBMs) are rich probabilistic models that have recently been applied to a wide range of problems, including collaborative filtering, classification, and modeling motion capture data. While much progress has been made in training non-conditional RBMs, these algorithms are not applicable to conditional models and there has been almost no work on training and generating predictions from conditional RBMs for structured output problems. We first argue that standard Con-trastive Divergence-based learning may not be suitable for training CRBMs. We then identify two distinct types of structured output prediction problems and propose an improved learning algorithm for each. The first problem type is one where the output space has arbitrary structure but the set of likely output configurations is relatively small, such as in multi-label classification. The second problem is one where the output space is arbitrarily structured but where the output space variability is much greater, such as in image denoising or pixel labeling. We show that the new learning algorithms can work much better than Contrastive Divergence on both types of problems.},
author = {Mnih, Volodymyr and Larochelle, Hugo and {E. Hinton}, Geoffrey},
file = {:E$\backslash$:/Master-Informatica/Semester II/Masterproef/thesis/bib/Conditional Restricted Boltzmann Machines for Structured Output Prediction.pdf:pdf},
journal = {Proceedings of the 27th Conference on Uncertainty in Artificial Intelligence, UAI 2011},
title = {{Instru{\c{c}}{\~{a}}o para emiss{\~{a}}o de Certid{\~{a}}o de Notifica{\c{c}}{\~{a}}o de Produtos Cosm{\'{e}}ticos}},
year = {2012}
}
@article{Singh2016,
author = {Singh, Gurkirt and Cuzzolin, Fabio},
file = {:E$\backslash$:/Master-Informatica/Semester II/Masterproef/thesis/bib/Untrimmed Video Classification for Activity Detection.pdf:pdf},
journal = {arXiv preprint arXiv:1607.01979},
title = {{Untrimmed Video Classification for Activity Detection: submission to ActivityNet Challenge}},
year = {2016}
}
@article{Poppe2010,
abstract = {Vision-based human action recognition is the process of labeling image sequences with action labels. Robust solutions to this problem have applications in domains such as visual surveillance, video retrieval and human-computer interaction. The task is challenging due to variations in motion performance, recording settings and inter-personal differences. In this survey, we explicitly address these challenges. We provide a detailed overview of current advances in the field. Image representations and the subsequent classification process are discussed separately to focus on the novelties of recent research. Moreover, we discuss limitations of the state of the art and outline promising directions of research. {\textcopyright} 2009 Elsevier B.V. All rights reserved.},
author = {Poppe, Ronald},
doi = {10.1016/j.imavis.2009.11.014},
file = {:E$\backslash$:/Master-Informatica/Semester II/Masterproef/thesis/bib/A survey on vision-based human action recognition.pdf:pdf},
isbn = {0262-8856},
issn = {02628856},
journal = {Image and Vision Computing},
keywords = {Action detection,Human action recognition,Motion analysis},
number = {6},
pages = {976--990},
publisher = {Elsevier B.V.},
title = {{A survey on vision-based human action recognition}},
volume = {28},
year = {2010}
}
@article{Wang2014a,
abstract = {We present an action recognition and detection system from temporally untrimmed videos by combining motion and appearance fea-tures. Motion and appearance are two kinds of complementary cues for human action understanding from video. For motion features, we adopt the Fisher vector representation with improved dense trajectories due to its rich descriptive capacity. For appearance feature, we choose the deep convolutional neural network activations due to its recent success in image based tasks. With this fused feature of iDT and CNN, we train a SVM classifier for each action class in the one-vs-all scheme. We report both the recognition and detection results of our system on Thumos 14 Challenge. From the results, we see that our method rank 4 th in the action recognition task and 2 nd in the action detection task.},
author = {Wang, Limin and Qiao, Yu and Tang, Xiaoou},
doi = {10.1016/j.amc.2018.01.039},
file = {:E$\backslash$:/Master-Informatica/Semester II/Masterproef/thesis/bib/WangQT{\_}Thumos14.pdf:pdf},
issn = {00963003},
journal = {ECCV2014 THUMOS Challenge},
pages = {1--6},
title = {{Action Recognition and Detection by Combining Motion and Appearance Features}},
url = {http://crcv.ucf.edu/THUMOS14/papers/CUHK{\&}SIAT.pdf},
year = {2014}
}
@article{Weinland2008a,
abstract = {In this paper, we address the problem of representing human actions using visual cues for the purpose of learning and recognition. Traditional approaches model actions as space-time representations which explicitly or implicitly encode the dynamics of an action through temporal dependencies. In contrast, we propose a new compact and efficient representation which does not account for such dependencies. Instead, motion sequences are represented with respect to a set of discriminative static key-pose exemplars and without modeling any temporal ordering. The interest is a time-invariant representation that drastically simplifies learning and recognition by removing time related information such as speed or length of an action. The proposed representation is equivalent to embedding actions into a space defined by distances to key-pose exemplars. We show how to build such embedding spaces of low dimension by identifying a vocabulary of highly discriminative exemplars using a forward selection. To test our representation, we have used a publicly available dataset which demonstrates that our method can precisely recognize actions, even with cluttered and non-segmented sequences.},
author = {Weinland, Daniel and Boyer, Edmond},
doi = {10.1109/CVPR.2008.4587731},
file = {:E$\backslash$:/Master-Informatica/Semester II/Masterproef/thesis/bib/Action Recognition Using Exemplar based Embedding.pdf:pdf},
isbn = {9781424422432},
journal = {26th IEEE Conference on Computer Vision and Pattern Recognition, CVPR},
number = {May},
title = {{Action recognition using exemplar-based embedding}},
year = {2008}
}
@inproceedings{JaeyongSung2012,
author = {{Jaeyong Sung} and Ponce, Colin and Selman, Bart and Saxena, Ashutosh},
booktitle = {2012 IEEE International Conference on Robotics and Automation},
doi = {10.1109/ICRA.2012.6224591},
file = {:E$\backslash$:/Master-Informatica/Semester II/Masterproef/thesis/bib/unstructured{\_}human{\_}activity{\_}learning.pdf:pdf},
isbn = {978-1-4673-1405-3},
month = {may},
pages = {842--849},
publisher = {IEEE},
title = {{Unstructured human activity detection from RGBD images}},
url = {http://ieeexplore.ieee.org/document/6224591/},
year = {2012}
}
@inproceedings{Laptev2008,
author = {Laptev, Ivan and Marszalek, Marcin and Schmid, Cordelia and Rozenfeld, Benjamin},
booktitle = {2008 IEEE Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2008.4587756},
file = {:E$\backslash$:/Master-Informatica/Semester II/Masterproef/thesis/bib/LaptevMarszalekSchmidRozenfeld-CVPR08-HumanActions.pdf:pdf},
isbn = {978-1-4244-2242-5},
month = {jun},
pages = {1--8},
publisher = {IEEE},
title = {{Learning realistic human actions from movies}},
url = {http://ieeexplore.ieee.org/document/4587756/},
year = {2008}
}
@inproceedings{Zhanpeng2013,
author = {{Zhanpeng Shao} and Li, Y F},
booktitle = {2013 IEEE International Conference on Robotics and Automation},
doi = {10.1109/ICRA.2013.6631253},
file = {:E$\backslash$:/Master-Informatica/Semester II/Masterproef/thesis/bib/zhanpengshao2013.pdf:pdf},
isbn = {978-1-4673-5643-5},
month = {may},
pages = {4749--4754},
publisher = {IEEE},
title = {{A new descriptor for multiple 3D motion trajectories recognition}},
url = {http://ieeexplore.ieee.org/document/6631253/},
year = {2013}
}
@inproceedings{Deboeverie2016,
abstract = {For a complete understanding of a wave energy conversion device, it is important to know how the proposed device moves in the water, how this motion can be measured, and to what extent the motion can be predicted or simulated. The magnitude and character of the motion has impacts on engi- neering issues and optimization of control parameters, as well as the theoretical understanding of the system. This paper presents real sea measurements of buoy motion and translator motion for a wave energy system using a linear generator. Buoy motion has been measured using two different systems: a land-based optical system and a buoy-based accelerometer system. The data have been compared to simulations from a Simulink model for the entire system. The two real sea measurements of buoy motion have been found to correlate well in the vertical direction, where the measured range of motion and the standard deviation of the position distributions differed with 3 and 4 cm, respectively. The difference in the horizontal direction ismore substantial. Themain reason for this is that the buoy rotation about its axis of symmetry was not measured. However, used together the two systems give a good understanding of buoy motion. In a first comparison, the simulations show good agreement with the measured motion for both translator and buoy.},
author = {Deboeverie, Francis and Roegiers, Sanne and Allebosch, Gianni and Veelaert, Peter and Philips, Wilfried},
booktitle = {IEEE Conference on Computational Intelligence and Games, CIG},
doi = {10.1109/CIG.2016.7860414},
file = {:E$\backslash$:/Master-Informatica/Semester II/Masterproef/thesis/bib/Human Gesture Classification by Brute-Force Machine Learning for Exergaming in Physiotherapy.pdf:pdf},
isbn = {9781509018833},
issn = {23254289},
title = {{Human gesture classification by brute-force machine learning for exergaming in physiotherapy}},
year = {2016}
}
@article{Li2018,
abstract = {This paper describes the development of a real-time Human-Robot Interaction (HRI) system for a service robot based on 3D human activity recognition and human-like decision mechanism. The Human-Robot Interactive (HRI) system, which allows one person to interact with a service robot using natural body language, collects sequences of 3D skeleton joints comprising rich human movement information about the user via Microsoft Kinect. This information is used to train a three-layer Long-Short-Term Memory (LSTM) network for human action recognition. The robot understands user intent based on an online LSTM network test, and responds to the user via movements of the robotic arm or chassis. Furthermore, the human-like decision mechanism is also fused into this process, which allows the robot to instinctively decide whether to interrupt the current task according to task priority. The framework of the overall system is established on the Robot Operating System (ROS) platform. The real-life activity interaction between our service robot and the user was conducted to demonstrate the effectiveness of developed HRI system.},
annote = {gebruiken simpele feature vector:
Fn = [x1, y1, z1, x2, y2, z2, ..., x15, y15, z15] (co{\"{o}}rdinaten van 15 joints)

Gebruiken ook neurale netten},
archivePrefix = {arXiv},
arxivId = {1802.00272},
author = {Li, Kang and Wu, Jinting and Zhao, Xiaoguang and Tan, Min},
eprint = {1802.00272},
file = {:E$\backslash$:/Master-Informatica/Semester II/Masterproef/thesis/bib/1802.00272.pdf:pdf},
month = {feb},
title = {{Real-Time Human-Robot Interaction for a Service Robot Based on 3D Human Activity Recognition and Human-mimicking Decision Mechanism}},
url = {http://arxiv.org/abs/1802.00272},
year = {2018}
}
@article{Zhao2017,
author = {Zhao, Yue and Xiong, Yuanjun and Wang, Limin and Wu, Zhirong and Tang, Xiaoou and Lin, Dahua},
file = {:E$\backslash$:/Master-Informatica/Semester II/Masterproef/thesis/bib/Temporal Action Detection with Structured Segment Networks.pdf:pdf},
title = {{Temporal Action Detection with Structured Segment Networks}},
year = {2017}
}
@article{Vemulapalli2014,
abstract = {Recently introduced cost-effective depth sensors coupled with the real-time skeleton estimation algorithm of Shotton et al. [16] have generated a renewed interest in skeleton-based human action recognition. Most of the existing skeleton-based approaches use either the joint locations or the joint angles to represent a human skeleton. In this paper, we propose a new skeletal representation that explicitly models the 3D geometric relationships between various body parts using rotations and translations in 3D space. Since 3D rigid body motions are members of the special Euclidean group SE(3), the proposed skeletal representation lies in the Lie group SE(3)×. . .×SE(3), which is a curved manifold. Using the proposed representation, human actions can be modeled as curves in this Lie group. Since classification of curves in this Lie group is not an easy task, we map the action curves from the Lie group to its Lie algebra, which is a vector space. We then perform classification using a combination of dynamic time warping, Fourier temporal pyramid representation and linear SVM. Experimental results on three action datasets show that the proposed representation performs better than many existing skeletal representations. The proposed approach also outperforms various state-of-the-art skeleton-based human action recognition approaches.},
author = {Vemulapalli, Raviteja and Arrate, Felipe and Chellappa, Rama},
doi = {10.1109/CVPR.2014.82},
file = {:E$\backslash$:/Master-Informatica/Semester II/Masterproef/thesis/bib/Human Action Recognition by Representing 3D Skeletons as Points in a Lie Group.pdf:pdf},
isbn = {9781479951178},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
keywords = {Action Recognition,Lie Groups,Special Euclidean Group},
pages = {588--595},
title = {{Human action recognition by representing 3D skeletons as points in a lie group}},
year = {2014}
}
@inproceedings{Vemulapalli2016,
author = {Vemulapalli, Raviteja and Chellappa, Rama},
booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2016.484},
file = {:E$\backslash$:/Master-Informatica/Semester II/Masterproef/thesis/bib/Vemulapalli{\_}Rolling{\_}Rotations{\_}for{\_}CVPR{\_}2016{\_}paper.pdf:pdf},
isbn = {978-1-4673-8851-1},
month = {jun},
pages = {4471--4479},
publisher = {IEEE},
title = {{Rolling Rotations for Recognizing Human Actions from 3D Skeletal Data}},
url = {http://ieeexplore.ieee.org/document/7780853/},
year = {2016}
}
@article{Wang2011,
author = {Wang, Heng and Kl, Alexander and Schmid, Cordelia and Cheng-lin, Liu and Wang, Heng and Kl, Alexander and Schmid, Cordelia and Recognition, Liu Cheng-lin Action and Kl, Alexander},
file = {:E$\backslash$:/Master-Informatica/Semester II/Masterproef/thesis/bib/Action recognition by dense trajectories.pdf:pdf},
journal = {Cvpr'11},
title = {{Action Recognition by Dense Trajectories}},
year = {2011}
}
@article{Martens2011,
abstract = {In this work we resolve the long-outstanding problem of how to effectively train recurrent neural networks (RNNs) on complex and difficult sequence modeling problems which may contain long-term data dependencies. Utilizing recent advances in the Hessian-free optimization approach (Martens, 2010), together with a novel damping scheme, we successfully train RNNs on two sets of challenging problems. First, a collection of pathological synthetic datasets which are known to be impossible for standard optimization approaches (due to their extremely long-term dependencies), and second, on three natural and highly complex real-world sequence datasets where we find that our method significantly outperforms the previous state-of-the-art method for training neural sequence models: the Long Short-term Memory approach of Hochreiter and Schmidhuber (1997). Additionally, we offer a new interpretation of the generalized Gauss-Newton matrix of sch (2002) which is used within the HF approach of Martens. Copyright 2011 by the author(s)/owner(s).},
author = {Martens, James and Sutskever, Ilya},
doi = {10.1145/346152.346166},
file = {:E$\backslash$:/Master-Informatica/Semester II/Masterproef/thesis/bib/532{\_}icmlpaper.pdf:pdf},
isbn = {978-1-4503-0619-5},
issn = {01635980},
journal = {Icml},
keywords = {Learning systems,Optimization},
pages = {1033--1040},
title = {{Learning recurrent neural networks with Hessian-free optimization}},
year = {2011}
}
@article{Cooper2005,
abstract = {A convenient representation of a video segment is a single "keyframe". Keyframes are widely used in applications such as non-linear browsing and video editing. With existing methods of keyframe selection, similar video segments result in very similar keyframes, with the drawback that actual differences between the segments may be obscured. We present methods for keyframe selection based on two criteria: capturing the similarity to the represented segment, and preserving the differences from other segment keyframes, so that different segments will have visually distinct representations. We present two discriminative keyframe selection methods, and an example of experimental results.},
author = {Cooper, Matthew and Foote, Jonathan},
doi = {10.1109/ICME.2005.1521470},
file = {:E$\backslash$:/Master-Informatica/Semester II/Masterproef/thesis/bib/Discriminative Techniques for Keyframe Selection.pdf:pdf},
isbn = {0780393325},
journal = {IEEE International Conference on Multimedia and Expo, ICME 2005},
number = {August 2005},
pages = {502--505},
title = {{Discriminative techniques for keyframe selection}},
volume = {2005},
year = {2005}
}
@article{Mettes2015,
abstract = {The goal of this paper is event detection and recounting using a representation of concept detector scores. Different from existing work, which encodes videos by averaging concept scores over all frames, we propose to encode videos using fragments that are discriminatively learned per event. Our bag-of-fragments split a video into semantically coherent fragment proposals. From training video proposals we show how to select the most discriminative fragment for an event. An encoding of a video is in turn generated by matching and pooling these discriminative fragments to the fragment proposals of the video. The bag-of-fragments forms an effective encoding for event detection and is able to provide a precise temporally localized event recounting. Furthermore, we show how bag-of-fragments can be extended to deal with irrelevant concepts in the event recounting. Experiments on challenging web videos show that i) our modest number of fragment proposals give a high sub-event recall, ii) bag-of-fragments is complementary to global averaging and provides better event detection, iii) bag-of-fragments with concept filtering yields a desirable event recounting. We conclude that fragments matter for video event detection and recounting.},
author = {Mettes, Pascal and Gemert, Jan C Van and Cappallo, Spencer and Mensink, Thomas and Snoek, Cees G M},
doi = {10.1145/2671188.2749404},
file = {:E$\backslash$:/Master-Informatica/Semester II/Masterproef/thesis/bib/Bag-of-Fragments$\backslash$; Selecting and encoding video fragments for event detection and recounting.pdf:pdf},
isbn = {9781450332743},
journal = {Icmr},
keywords = {bag-of-fragments,criminative fragments,dis-,event detection,event recounting},
pages = {427--434},
title = {{Bag-of-Fragments : Selecting and Encoding Video Fragments for Event Detection and Recounting}},
year = {2015}
}
@inproceedings{Li2010,
abstract = {This paper presents a method to recognize human actions from sequences of depth maps. Specifically, we employ an action graph to model explicitly the dynamics of the actions and a bag of 3D points to characterize a set of salient postures that correspond to the nodes in the action graph. In addition, we propose a simple, but effective projection based sampling scheme to sample the bag of 3D points from the depth maps. Experimental results have shown that over 90{\%} recognition accuracy were achieved by sampling only about 1{\%} 3D points from the depth maps. Compared to the 2D silhouette based recognition, the recognition errors were halved. In addition, we demonstrate the potential of the bag of points posture model to deal with occlusions through simulation.},
author = {Li, Wanqing and Zhang, Zhengyou and Liu, Zicheng},
edition = {2010 IEEE },
file = {:E$\backslash$:/Master-Informatica/Semester II/Masterproef/thesis/bib/Action recognition based on a bag of 3D points.pdf:pdf},
month = {jun},
pages = {9--14},
publisher = {IEEE},
title = {{Action recognition based on a bag of 3d points}},
url = {https://www.microsoft.com/en-us/research/publication/action-recognition-based-bag-3d-points/},
year = {2010}
}
@inproceedings{Yuan2016,
author = {Yuan, Jun and Ni, Bingbing and Yang, Xiaokang and Kassim, Ashraf},
doi = {10.1109/CVPR.2016.337},
file = {:E$\backslash$:/Master-Informatica/Semester II/Masterproef/thesis/bib/Temporal Action Localization with Pyramid of Score Distribution Features.pdf:pdf},
pages = {3093--3102},
title = {{Temporal Action Localization with Pyramid of Score Distribution Features}},
year = {2016}
}
@article{Gu2010,
abstract = {A common viewpoint-free framework that fuses pose recovery and classification for action and gait recognition is presented in this paper. First, a markerless pose recovery method is adopted to automatically capture the 3-D human joint and pose parameter sequences from volume data. Second, multiple configuration features (combination of joints) and movement features (position, orientation, and height of the body) are extracted from the recovered 3-D human joint and pose parameter sequences. A hidden Markov model (HMM) and an exemplar-based HMM are then used to model the movement features and configuration features, respectively. Finally, actions are classified by a hierarchical classifier that fuses the movement features and the configuration features, and persons are recognized from their gait sequences with the configuration features. The effectiveness of the proposed approach is demonstrated with experiments on the Institut National de Recherche en Informatique et Automatique Xmas Motion Acquisition Sequences data set.},
author = {Gu, Junxia and Ding, Xiaoqing and Wang, Shengjin and Wu, Youshou},
doi = {10.1109/TSMCB.2010.2043526},
issn = {10834419},
journal = {IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics},
keywords = {Action recognition,exemplar-based hidden Markov model (HMM),gait recognition,three-dimensional (3-D) human joints},
title = {{Action and gait recognition from recovered 3-D human joints}},
year = {2010}
}
@article{Muller2006,
abstract = {This paper presents new methods for automatic classification and retrieval of motion capture data facilitating the identification of logically related motions scattered in some database. As the main ingredient, we introduce the concept of motion templates (MTs), by which the essence of an entire class of logically related motions can be captured in an explicit and semantically interpretable matrix representation. The key property of MTs is that the variable aspects of a motion class can be automatically masked out in the comparison with unknown motion data. This facilitates robust and efficient motion retrieval even in the presence of large spatio-temporal variations. Furthermore, we describe how to learn an MT for a specific motion class from a given set of training motions. In our extensive experiments, which are based on several hours of motion data, MTs proved to be a powerful concept for motion annotation and retrieval, yielding accurate results even for highly variable motion classes such as cartwheels, lying down, or throwing motions.},
author = {M{\"{u}}ller, Meinard and R{\"{o}}der, Tido},
isbn = {3-905673-34-7},
journal = {SCA},
title = {{Motion Templates for Automatic Classification and Retrieval of Motion Capture Data}},
year = {2006}
}
@article{Gahlot2016,
annote = {brol paper},
author = {Gahlot, Ayushi and Purvi, Agarwal and Akshya, Agarwal},
file = {:E$\backslash$:/Master-Informatica/Semester II/Masterproef/thesis/bib/Skeleton based Human Action Recognition using Kinect.pdf:pdf},
journal = {International Journal of Computer Applications},
keywords = {action recognition,hmm,microsoft kinect sensor,pose estimation,skeletal tracking},
pages = {9--13},
title = {{Skeleton based Human Action Recognition using Kinect}},
year = {2016}
}
@article{Ohn-Bar2013,
abstract = {We propose a set of features derived from skeleton track- ing of the human body and depth maps for the purpose of action recognition. The descriptors proposed are easy to implement, produce relatively small-sized feature sets, and the multi-class classification scheme is fast and suitable for real-time applications. We intuitively characterize actions using pairwise affinities between view-invariant joint angles features over the performance of an action. Additionally, a new descriptor for spatio-temporal feature extraction from color and depth images is introduced. This descriptor in- volves an application of a modified histogram of oriented gradients (HOG) algorithm. The application produces a feature set at every frame, and these features are collected into a 2D array which then the same algorithm is applied to again (the approach is termed HOG2). Both feature sets are evaluated in a bag-of-words scheme using a linear SVM, showing state-of-the-art results on public datasets from dif- ferent domains of human-computer interaction.},
author = {Ohn-Bar, Eshed and Trivedi, Mohan M.},
doi = {10.1109/CVPRW.2013.76},
file = {:E$\backslash$:/Master-Informatica/Semester II/Masterproef/thesis/bib/Joint Angles Similarities and HOG for Action Recognition.pdf:pdf},
isbn = {9780769549903},
issn = {21607508},
journal = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
pages = {465--470},
title = {{Joint angles similarities and HOG2for action recognition}},
year = {2013}
}
@article{Ramage2007,
abstract = {How can we apply machine learning to data that is represented as a sequence of observations over time? For instance, we might be interested in discovering the sequence of words that someone spoke based on an audio recording of their speech. Or we might be interested in annotating a sequence of words with their part-of-speech tags. These notes provides a thorough mathematical introduction to the concept of Markov Models ? a formalism for reasoning about states over time ? and Hidden Markov Models ? where we wish to recover a series of states from a series of observations. The ?nal section includes some pointers to resources that present this material from other perspectives.},
author = {Ramage, Daniel},
file = {:E$\backslash$:/Master-Informatica/Semester II/Masterproef/thesis/bib/Hidden Markov Models Fundamentals.pdf:pdf},
title = {{Hidden Markov Models Fundamentals}},
url = {http://see.stanford.edu/materials/aimlcs229/cs229-hmm.pdf},
year = {2007}
}
@phdthesis{Kerkhove2017,
abstract = {Self-driving cars are a hot topic and many manufacturers spend significant amounts of time on R{\&}D to be the first to bring them to the masses. Under the hood however many challenges need to be overcome, one of the most important one being safety for both the driver and pedestrians. It is crucial that pedestrians can be detected at all times with various sensors without impeding the autonomous behaviour of the vehicle. In this dissertation we evaluate which feature descriptors can be used on the available RGB, depth (LIDAR) and thermal sen- sor data and which combinations synergize well together. As detection needs to be done as fast as possible, we've analyzed the evaluation speed of the feature descriptors and researched which techniques can be used to further speed up evaluation. We also introduce a more real life usable categorization of the detection results based on the risk the vehicle poses. Finally we take a look at which features are the most discriminative and in which areas and analyze which cases prevent a correct detection. Keywords:},
author = {Kerkhove, Dwight},
file = {:E$\backslash$:/Master-Informatica/Semester II/Masterproef/thesis/bib/Pedestrian detection using heterogeneous sensor data.pdf:pdf},
keywords = {features,machine learning,object detection,sensor fusion},
school = {Ghent University},
title = {{Pedestrian detection using heterogeneous sensor data}},
year = {2017}
}
@article{Zhang2015,
abstract = {Statistical classification of actions in videos is mostly performed by extracting relevant features, particularly covariance features, from image frames and studying time series associated with temporal evolutions of these features. A natural mathematical representation of activity videos is in form of parameterized trajectories on the covariance manifold, i.e. the set of symmetric, positive-definite matrices (SPDMs). The variable execution-rates of actions implies variable parameterizations of the resulting trajectories, and complicates their classification. Since action classes are invariant to execution rates, one requires rate-invariant metrics for comparing trajectories. A recent paper represented trajectories using their transported square-root vector fields (TSRVFs), defined by parallel translating scaled-velocity vectors of trajectories to a reference tangent space on the manifold. To avoid arbitrariness of selecting the reference and to reduce distortion introduced during this mapping, we develop a purely intrinsic approach where SPDM trajectories are represented by redefining their TSRVFs at the starting points of the trajectories, and analyzed as elements of a vector bundle on the manifold. Using a natural Riemannain metric on vector bundles of SPDMs, we compute geodesic paths and geodesic distances between trajectories in the quotient space of this vector bundle, with respect to the re-parameterization group. This makes the resulting comparison of trajectories invariant to their re-parameterization. We demonstrate this framework on two applications involving video classification: visual speech recognition or lip-reading and hand-gesture recognition. In both cases we achieve results either comparable to or better than the current literature.},
archivePrefix = {arXiv},
arxivId = {1503.06699},
author = {Zhang, Zhengwu and Su, Jingyong and Klassen, Eric and Le, Huiling and Srivastava, Anuj},
eprint = {1503.06699},
file = {:E$\backslash$:/Master-Informatica/Semester II/Masterproef/thesis/bib/Action Recognition Using Rate-Invariant Analysis of Skeletal Shape Trajectories.pdf:pdf},
number = {May},
title = {{Video-Based Action Recognition Using Rate-Invariant Analysis of Covariance Trajectories}},
url = {http://arxiv.org/abs/1503.06699},
year = {2015}
}
@article{Weinland2010,
abstract = {Action recognition has become a very important topic in computer vision, with many fundamental applications, in robotics, video surveillance, human computer interaction, and multimedia retrieval among others and a large va- riety of approaches have been described. The purpose of this survey is to give an overview and categorization of the approaches used. We concentrate on approaches that aim on classification of full-body motions, such as kicking, punching, waving, etc. and we categorize them according to how they repre- sent the spatial and temporal structure of actions; how they segment actions from an input stream of visual data; and how they learn a view-invariant representation of actions.},
author = {Weinland, Daniel and Ronfard, R{\'{e}}mi and Boyer, Edmond},
file = {:E$\backslash$:/Master-Informatica/Semester II/Masterproef/thesis/bib/A survey of vision-based methods for action representation, segmentation and recognition.pdf:pdf},
journal = {Computer Vision and Image Understanding},
keywords = {action/activity recognition,computer vision,survey},
pages = {224--241},
title = {{A survey of vision-based methods for action representation , segmentation and recognition}},
volume = {115},
year = {2011}
}
@article{Patrona2018,
abstract = {A novel framework, for real-time action detection, recognition and evaluation of motion capture data, is presented in this paper. Pose and kinematics information is used for data description. Automatic and dynamic weighting, altering joint data significance based on action involvement, and Kinetic energy-based descriptor sampling are employed for efficient action segmentation and labelling. The automatically segmented and recognized action instances are subsequently fed to the framework action evaluation component, which compares them with the corresponding reference ones, estimating their similarity. Exploiting fuzzy logic, the framework subsequently gives semantic feedback with instructions on performing the actions more accurately. Experimental results on MSR-Action3D and MSRC12 benchmarking datasets and a new, publicly available one, provide evidence that the proposed framework compares favourably to state-of-the-art methods by 0.5–6{\%} in all three datasets, showing that the proposed method can be effectively used for unsupervised gesture/action training.},
author = {Patrona, Fotini and Chatzitofis, Anargyros and Zarpalas, Dimitrios and Daras, Petros},
doi = {10.1016/j.patcog.2017.12.007},
file = {:E$\backslash$:/Master-Informatica/Semester II/Masterproef/thesis/bib/Motion analysis$\backslash$; Action detection, recognition and evaluation based on motion capture data.pdf:pdf},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Automatic joint/angle weighting,Kinect,Kinetic energy,Motion evaluation,Online human action detection,Online human action recognition,Skeleton data},
pages = {612--622},
publisher = {Elsevier Ltd},
title = {{Motion analysis: Action detection, recognition and evaluation based on motion capture data}},
url = {https://doi.org/10.1016/j.patcog.2017.12.007},
volume = {76},
year = {2018}
}
@inproceedings{Chaudhry2013,
author = {Chaudhry, Rizwan and Ofli, Ferda and Kurillo, Gregorij and Bajcsy, Ruzena and Vidal, Rene},
booktitle = {2013 IEEE Conference on Computer Vision and Pattern Recognition Workshops},
doi = {10.1109/CVPRW.2013.153},
file = {:E$\backslash$:/Master-Informatica/Semester II/Masterproef/thesis/bib/Bio-inspired Dynamic 3D Discriminative Skeletal Features for Human Action Recognition.pdf:pdf},
isbn = {978-0-7695-4990-3},
month = {jun},
pages = {471--478},
publisher = {IEEE},
title = {{Bio-inspired Dynamic 3D Discriminative Skeletal Features for Human Action Recognition}},
url = {http://ieeexplore.ieee.org/document/6595916/},
year = {2013}
}
@article{Devi2015,
abstract = {Gesture recognition means the identification of different expressions of human body parts to express the idea, thoughts and emotion. It is a multi-disciplinary research area. The application areas of gesture recognition have been spreading very rapidly in our real-life activities including dance gesture recognition. Dance gesture recognition means the recognition of meaningful expression from the different dance poses. Today, research on dance gesture recognition receives more and more attention throughout the world. The automated recognition of dance gestures has many applications. The motive behind this survey is to present a comprehensive survey on automated dance gesture recognition with emphasis on static hand gesture recognition. Instead of whole body movement, we consider human hands because human hands are the most flexible part of the body and can transfer the most meaning. A list of research issues and open challenges is also highlighted.},
author = {Devi, Mampi and Saharia, Sarat and D.K.Bhattacharyya, D.K.Bhattacharyya},
doi = {10.5120/21696-4803},
file = {:E$\backslash$:/Master-Informatica/Semester II/Masterproef/thesis/bib/Dance Gesture Recognition - A Survey.pdf:pdf},
journal = {International Journal of Computer Applications},
number = {5},
pages = {19--26},
title = {{Dance Gesture Recognition: A Survey}},
volume = {122},
year = {2015}
}
@article{Schuldt2004,
abstract = {Local space-time features capture local events in video and can be adapted to the size, the frequency and the velocity of moving patterns. In this paper, we demonstrate how such features can be used for recognizing complex motion patterns. We construct video representations in terms of local space-time features and integrate such representations with SVM classification schemes for recognition. For the purpose of evaluation we introduce a new video database containing 2391 sequences of six human actions performed by 25 people in four different scenarios. The presented results of action recognition justify the proposed method and demonstrate its advantage compared to other relative approaches for action recognition.},
archivePrefix = {arXiv},
arxivId = {1505.04868},
author = {Schuldt, Christian and Barbara, Laptev and Stockholm, Se-},
doi = {10.1109/ICPR.2004.1334462},
eprint = {1505.04868},
file = {:E$\backslash$:/Master-Informatica/Semester II/Masterproef/thesis/bib/2004{\_}icpr{\_}schuldt.pdf:pdf},
isbn = {0769521282},
issn = {10514651},
journal = {Pattern Recognition, 2004. ICPR 2004. Proceedings of the 17th International Conference on},
pages = {32--36},
pmid = {12171414},
title = {{Recognizing Human Actions : A Local SVM Approach ∗ Dept . of Numerical Analysis and Computer Science}},
volume = {3},
year = {2004}
}
@article{Yeung2015,
abstract = {In this work we introduce a fully end-to-end approach for action detection in videos that learns to directly predict the temporal bounds of actions. Our intuition is that the process of detecting actions is naturally one of observation and refinement: observing moments in video, and refining hypotheses about when an action is occurring. Based on this insight, we formulate our model as a recurrent neural network-based agent that interacts with a video over time. The agent observes video frames and decides both where to look next and when to emit a prediction. Since backpropagation is not adequate in this non-differentiable setting, we use REINFORCE to learn the agent's decision policy. Our model achieves state-of-the-art results on the THUMOS'14 and ActivityNet datasets while observing only a fraction (2{\%} or less) of the video frames.},
archivePrefix = {arXiv},
arxivId = {1511.06984},
author = {Yeung, Serena and Russakovsky, Olga and Mori, Greg and Fei-Fei, Li},
doi = {10.1109/CVPR.2016.293},
eprint = {1511.06984},
file = {:E$\backslash$:/Master-Informatica/Semester II/Masterproef/thesis/bib/End-to-end Learning of Action Detection from Frame Glimpses in Videos.pdf:pdf},
isbn = {978-1-4673-8851-1},
issn = {10636919},
pmid = {23692016},
title = {{End-to-end Learning of Action Detection from Frame Glimpses in Videos}},
url = {http://arxiv.org/abs/1511.06984},
year = {2015}
}
@article{Uddin2011,
abstract = {This paper presents a novel approach of Human Activity Recognition (HAR) using the joint angles of the human body in 3-D. From each pair of activity video images acquired by a stereo camera, the body joint angles are estimated by co-registering a 3-D body model to the stereo information: our approach uses no attached sensors on the human. The estimated joint angle features from the time-sequential activity video frames are then mapped into codewords to generate a sequence of discrete symbols for a Hidden Markov Model (HMM) of each activity. With these symbols, each activity HMM is trained and used for activity recognition. The performance of our joint angle-based HAR has been compared to that of the conventional binary silhouette-based HAR, producing significantly better results in the recognition rate: especially for those activities that are not discernible with the conventional approaches.},
author = {Uddin, Md Zia and Thang, Nguyen Duc and Kim, Jeong Tai and Kim, Tae Seong},
doi = {10.4218/etrij.11.0110.0314},
file = {:E$\backslash$:/Master-Informatica/Semester II/Masterproef/thesis/bib/Human Activity Recognition Using Body Joint-Angle Features and Hidden Markov Model.pdf:pdf},
issn = {12256463},
journal = {ETRI Journal},
keywords = {Body-joint-angle features,Hidden Markov model(HMM),Human activity recognition (HAR)},
number = {4},
pages = {569--579},
title = {{Human activity recognition using body joint-angle features and hidden Markov model}},
volume = {33},
year = {2011}
}
