\chapter{notities}

\section{papers}
\subsubsection{Actieherkenning met skeletdata}
\begin{itemize}
	\item Bron \cite{Vemulapalli2014}
	\begin{itemize}
		\item \textbf{Inleiding}: Nieuwe representatie voor skelet dat de 3D verhoudingen tussen lichaamsdelen expliciet modelleert, gebruik makend van rotaties en translaties. Zij maken een allereerst een onderscheid tussen \textit{joint-based} en \textit{body-based} actieherkenning
		\begin{itemize}
			\item Joint-based: Het menselijk skelet is gewoon een verzameling van punten. Aantal verschillende methoden:
			\begin{itemize}
				\item Gebruik maken van enkel de joints posities \cite{Hussein2011}, \cite{Lv2006}
				\item Gebruik maken van een assenstelsel \cite{Xia2012}
				\item Paarsgewijze relatieve joint posities \cite{Wang2012b}, \cite{Yang2012}
			\end{itemize}
		
			\item Body-based: Het menselijk skelet is een graaf van joints. \cite{Ofli2012}, \cite{Chaudhry2013}
		\end{itemize}

		\item \textbf{Methode:}
		\begin{enumerate}
			\item Een skeletjoint kan met een ander skeletjoint beschreven worden met behulp van rotaties en translaties. M.a.w: de \textit{relatieve geometrie} kan beschreven worden tussen twee punten.
			\item Zulke rotaties en translaties in drie dimensies maken deel uit van de \textit{speciale Euclidische groep} SE(3).
			\item De relatieve geometrie tussen twee joints is een punt in SE(3). Het hele skelet is een punt in $SE(3) \times ... \times SE(3)$.
			\item Acties kunnen nu gerepresenteerd worden door een curve in $SE(3) \times ... \times SE(3)$. Op die curves wordt dan classificatie uitgevoerd.
			\item De groep $SE(3) \times ... \times SE(3)$ is niet vlak, zodat traditionele classificatiemethoden zoals SVM analyse niet direct werken. Daarom wordt de groep eerst gemapt op zijn algebra 
		\end{enumerate}
		\item \textbf{Gebruikte datasets:}
		\begin{itemize}
			\item MSR-Action3D dataset
			\item UTKinect-Action dataset
			\item Florence3D-Action dataset
		\end{itemize}
		\item \textbf{Toekomst:}
		\begin{itemize}
			\item Zij bepalen de relatieve positie tussen elk paar van joints, maar elke actie wordt maar gekarakteriseerd door een specifieke verzameling van joints. Ze zoeken een manier om automatisch te bepalen welke verzameling van joints samen horen tijdens een actie. 
			
			{\color{green}Wordt opgelost in \cite{Wang2012b} met de actionlet mining}
			\item Zij hebben dit ook maar uitgetest op één enkele persoon. Ze willen dit ook uitbreiden naar een multi-persoon model.
		\end{itemize}
	\end{itemize}

	\item Bron \cite{Hussein2011}
	\begin{itemize}
		\item \textbf{Inleiding}: Gebruik maken van  3D skeletdata in combinatie met de covariantiematrix in de tijd om het temporale aspect te modelleren. Eerst kaderen ze drie problemen aan bij actieherkenning:
		\begin{enumerate}
			\item \textbf{Data Capture:} / onbelangrijk
			\item \textbf{Feature Descriptors:} Het vinden van betrouwbare en discriminerende feature descriptors. Vaak voorkomende vormen van feature descriptors voor actieherkenning zijn: \textit{whole sequence}, \textit{individual frames} (\cite{Wang2012b}, \cite{Xia2012}) en \textit{interest points} (\cite{Laptev2003}, \cite{Gowayyed2013}) descriptors. De laatste twee hebben nood aan extra stappen om het temporale aspect te modelleren.
			\item \textbf{Action Modeling:} Sequentieanalyse wordt bijvoorbeeld geïmplementeerd met \gls{ac:hmm} \cite{Xia2012} of \gls{ac:crf} \cite{Han2010}. Soms wordt er ook recurrent neural networks \cite{Martens2011} of conditional restricted boltzman machines \cite{Mnih2012} gebruikt, maar deze hebben een groot aantal parameters met als gevolg dat er te veel trainingdata en tijd nodig is om deze te trainen.
		\end{enumerate}
		\item \textbf{Methode:}
		\begin{enumerate}
			\item (literatuur) Een covariantiematrix voor een verzameling van $N$ willekeurige variabelen is een $N\times N$ matrix waarvan de elementen de covariantie zijn tussen elk paar variabele. Stel $\textbf{X}$ een vector met $N$ willekeurige variabelen. De covariantiematrix wordt:
			$$cov(\textbf{X}) = cov(X_i, X_j) = E[(X_i - E[X_i])(X_j - E[X_j])]$$
			
			Zo een matrix bevat informatie over de vorm van de gezamenlijke kans voor deze variabelen.
			 Zo een covariantiematrix werd origineel gebruikt voor objectdetectie en voetgangerdetectie. Tegenwoordig wordt het ook gebruikt.
			\item Stel $K$ joints, tijdsstip $t$ en frame $i$: dan kan elke joint weergegeven door hun coordinaten: $(x_i^{(t)}, y_i^{(t)}, z_i^{(t)})$
			
			Stel $\textbf{S}$ de vector van alle joint locaties:
			
			$$\textbf{S} = [x_1, ..., x_{K},y_1, ..., y_{K},z_1, ..., z_{K}]$$
			
			De covariantiedescriptor is dan $cov(\textbf{S})$. Deze is symmetrisch dus mag enkel de bovenste driehoek gebruikt worden. Deze descriptor noemen ze \textbf{COV3DJ}.
			
			\item Het temporale aspect wordt genegeerd. Daarom gebruiken ze een hiëararchy van Cov3DJs. Deze kunnen overlappend zijn.
		\end{enumerate}
		\item \textbf{Gebruikte datasets:}
		\begin{itemize}
			\item MSR-Action3D Dataset
			\item MSRC-12 Kinect Gesture Dataset
			\item HDM05-MoCap Dataset
		\end{itemize}
		\item \textbf{Toekomst:}
		\begin{itemize}
			\item De methode is wel schaal en translatie invariant, maar niet rotatie of reflectie invariant. 
			
			Rotatie zou kunnen opgelost worden door rotatie-invariante features te nemen, zoals hoeken.
			
			Reflectie is moeilijker op te lossen, en soms ook niet gewenst.
			
			\item Een meer flexibele subdivisie van hiërarchie zou performantie kunnen verhogen.
			
			
		\end{itemize}
	\end{itemize}
	\item Bron \cite{Wang2012b}
	\begin{itemize}
		\item \textbf{Inleiding:} Ze ontwikkelen een manier om intra-klasse variantie op te lossen met dieptebeelden + skeletbeelden via \cite{Shotton2011}. Ze geven ook nieuwe features die voor dieptedata werken. Ze vermelden dat features sensorafhankelijk zijn. Bij kleurenbeelden wordt bijvoorbeeld \gls{ac:stip} \cite{Laptev2003} gebruikt om interessepunten te vinden, en gebruiken \gls{ac:hof} \cite{Laptev2010} of \gls{ac:hog} \cite{Dalal2010}. Voor dieptebeelden werken zo features niet. 
		
		Om het temporale aspect te modelleren worden \gls{ac:hmm} \cite{Lv2006} gebruikt, of \gls{ac:crf} \cite{Han2010}. Andere manier is \gls{ac:dtw} \cite{Muller2006}, maar dat vereist een goede metriek om twee frames te vergelijken. Voor cyclische acties lijkt DTW ook niet goed. Daarom gebruiken zij \gls{ac:ftp}.
		\item \textbf{Methode:}
		\begin{enumerate}
			\item Op basis van dieptedata en 3D joint posities bouwen zij een nieuwe feature \gls{ac:lop} op. Elke 3D joint wordt geassocieerd met zo een \gls{ac:lop}.
			\item Zij gebruiken niet individuele joints, maar gebruiken de relatieve positie. Een joint $i$ heeft 3 (genormaliseerde) coördinaten $p_i(t) = (x_i(t), y_i(t), z_i(t))$ op een frame $t$.
			
			Ze berekenen $\textbf{p}_{ij} = \textbf{p}_i - \textbf{p}_j$, de relatieve positie van een joint $i$ met elke andere joint $j$. Op die manier wordt er voor elke joint $i$ de feature bepaalt:
			
			$$\textbf{p}_i = \{\textbf{p}_{ij} | i \neq j \}$$
			
			Tussen elk paar joints berekenen is overbodig, ze introduceren daarom actionlet mining om enkel de joints te selecteren die nuttig zijn voor de classificatie.
 			
 			\item De bijkomende feature, \gls{ac:lop} dient om de lokale diepte voor de joints te modelleren. Ze construeren een drie-dimensionale puntenwolk rond een bepaalde joint.
 			
 			Voor frame $t$ kan voor elke joint $j$ een puntenwolk opgebouwd worden als volgt:
 			
 			\begin{enumerate}
 				\item De lokale regio van $j$ wordt gepartitioneerd in een $N_x \times N_y \times N_z$ grid. Elke bin in de grid heeft  $(S_x, S_y, S_z)$ pixels.
 				\item Het aantal punten voor $t$ dat in elke bin $b_{xyz}$ zit wordt geteld. Er wordt een sigmoid normalisatiefunctie toegepast om de feature $o_{xyz}$ te bekomen voor $b_{xyz}$. De \textit{local occupancy information} voor die bin wordt dan:
 				
 				$$o_{xyz} = \delta(\sum_{q \in bin_{xyz}} I_q)$$
 				
 				met $I_q = 1$ als er zich een punt bevindt op locatie $q$ en 0 anders en $\sigma(x) = \frac{1}{1 + e^{-\beta x}}$ 
 					
 				\item Voor elke frame $t$ zijn er twee features: De 3D posities $\textbf{p}_i[t]$ en de LOP features $\textbf{o}_i[t]$. Via \gls{ac:ftp} wordt de temporale dynamiek gemodellerd.
 			\end{enumerate} 
		\end{enumerate}
		\item \textbf{Gebruikte datasets:}
		\begin{itemize}
			\item MSRDailyActivity3D Dataset
			\item CMU MoCap Dataset
		\end{itemize}
		\item \textbf{Toekomst:}
		\begin{itemize}
			\item Gebruik van actionlets om meer complexere acties te begrijpen.
		\end{itemize}
	\end{itemize}
	
	\item Bron \cite{Yang2012}
	\begin{itemize}
		\item \textbf{Inleiding:} Ze bouwen een nieuwe feature op, \textit{eigenjoints}, gebaseerd op de positionele verschillende van joints. Ze gebruiken naïve-bayes-nearest-neighbor classifier voor multiklasse actieclassificatie.
		\item \textbf{Methode:}
		\begin{enumerate}
			
			\item Er worden drie features opgebouwd:
			\begin{enumerate}
				\item \textbf{Posture feature}
				
				Voor een frame $c$, wordt de posture feature gedefinieerd als
				
				$$f_{cc} = {x_i - x_j | i,j=1,2,...,N; i\neq j}$$
				
				\item \textbf{Motion feature}
				
				Voor een frame $c$, het verschil wordt genomen tussen de paarsgewijze joints van frame $c$ en een voor de vorige frame $p$.
				
				$$f_{cp} = \{x_i^c - x_j^p | x_i^c \in X_c; x_j^p \in X_p\}$$
					
				\item \textbf{Offset feature}
				
				
				De algemene dynamiek wordt bepaalt door de de motion feature toe te passen, met $p$ de initiële frame $i$. Deze frame $i$ modelleert de neutrale positie.
				$$f_{cp} = \{x_i^c - x_j^i | x_i^c \in X_c; x_j^i \in X_p\}$$
			\end{enumerate}
			\item Deze 3 features worden geconcateneerd
			
			$$f_c = [f_{cc}, f_{cp}, f_{ci}]$$
			
			De feature $f_c$ wordt lineair genormaliseerd, $f_{norm}$, zodat elk attribuut een bereik heeft van $[-1, +1]$.
			
			\item Met \gls{ac:pca} wordt op $f_{norm}$ de ruis en redundantie vermindert, en worden de eigenjoints bekomen. De eerste 128 eigenwaarden hebben een gewicht van 95\%
		\end{enumerate}
		\item \textbf{Gebruikte datasets:}
		\begin{itemize}
			\item MSR Action3D dataset
		\end{itemize}
		\item \textbf{Toekomst:}
		Meer testpersonen zoeken om herkenning met een cross-subject test uit te voeren.
	\end{itemize}

	\item Bron \cite{Ofli2012}
	\begin{itemize}
		\item \textbf{Inleiding:} Ze vermelden dat elke persoon een actie op een verschillende manier kan uitvoeren, maar dat zij toch dezelfde joints gebruiken om die actie uit te voeren. Ze berekenen de \textit{relatieve informativeness} van alle joints in een temporale window tijdens een actie. Een joint is bv belangrijk als het de hoogste variantie heeft door de grootste verandering in hoek.
		\item \textbf{Methode:} 
		\begin{enumerate}
			\item Ze berekenen de hoek tussen twee ledematen en gebruiken de tijdsreeks van deze hoeken als motion data. De vector $\textbf{a}^i$ bevat de tijdsreeks van de hoeken voor joint $i$ voor $T$ frames.
			\item Een actie kan dan gezien worden als de verzameling van alle vectoren
			
			$$A = [\textbf{a}^1\; \textbf{a}^2\; ...\; \textbf{a}^J]$$
			
			$J$ is het aantal joints,	dus $A$ is een $T \times J$ matrix.
			
			\item Uit $A$ kan bijvoorbeeld het gemiddelde, variantie en maximale hoeksnelheid berekent worden voor elke joint, hier aangeduid met de operatie $\mathcal{O}$, die een bewerking uitvoert op een vector en een scalaire waarde teruggeeft:
			
			$$\mathcal{O}(\textbf{a}) : \mathcal{R}^{|\textbf{a}|} \mapsto \mathcal{R}$$
			
			\item Een actiesequentie kan nu als feature beschreven worden
			
			$$\textbf{F} = \{\textbf{f}_1, \textbf{f}_2, ..., \textbf{f}_{N_{s}}\}$$
			met
			$$\textbf{f}_k = [\mathcal{O}(\textbf{a}_k^1)\; \mathcal{O}(\textbf{a}_k^2) \; ... \; \mathcal{O}(\textbf{a}_k^J)]$$
			
			Deze worden gesorteerd en zo wordt de SMIJ feature bekomen:
			$$ SMIJ = \{\{ idof(sort(\textbf{f}_k) ,n)\}_{k=1,...,N_s}\}_{n=1,...,N}$$
			In other words, the SMIJ features represent an action sequence by encoding the set of $N$ most informa- tive joints at a specific time instant (by rank-ordering and keeping the top-ranking $N$ joints) as well as the temporal evolution of the set of the most informative joints through- out the action sequence (by preserving the temporal order of
			the top-ranking $N$ joints).
		\end{enumerate}
		\item \textbf{Gebruikte datasets:}
		\begin{itemize}
			\item /
			\item HDM05 database
			\item MSR Action3D dataset
		\end{itemize}
		\item \textbf{Toekomst:}
		\begin{itemize}
			\item {\color{red}Geen. Ze geloven dat hun methode het meest ideale is voor praktische toepassingen.}
		\end{itemize}
	\end{itemize}
	\item Bron \cite{Vemulapalli2016}
	\begin{itemize}	
		\item \textbf{Inleiding:} Ze vermelden ook het verschil tussen joint based en body based: 
		\begin{itemize}
			\item \textbf{Joint based:}
			\begin{itemize}
				\item individuele joints \cite{Gowayyed2013}, \cite{Gu2010}, \cite{Hussein2011}
				\item Pairwise joints \cite{Wang2012b}, \cite{Wang2013}, \cite{Wei2013}, \cite{Yang2012}
				\item Assenstelsel \cite{Xia2012}, \cite{Zhanpeng2013}
			\end{itemize}
			\item  \textbf{Part based:}
			\begin{itemize}
				\item Hoeken: \cite{Ofli2012}, \cite{Ohn-Bar2013}, \cite{JaeyongSung2012}
				\item Bio-inspired : \cite{Chaudhry2013}
				\item relative geometry : \cite{Vemulapalli2014}
			\end{itemize}
		\end{itemize}
		\item \textbf{Uitleg over de groepen:}
		\begin{itemize}
			\item \textbf{$SO_n$}
			
			De speciale orthogonale groep (rotatiegroep) $SO_n$ is een groep gevormd door de verzameling van $n \times n$ matrices $R$ die voldoen aan
			
			$$R^TR = RR^T = I_n$$
			met $|R| = 1$. De elementen van $SO_n$ bewerken punten in $R^n$ via matrix-vector vermenigvuldiging:
			
			$$SO_n \circ R^n \rightarrow R^n, R \circ p = Rp$$
		\end{itemize}
		\item \textbf{Methode:} 
		\begin{enumerate}
			\item Ze representeren een 3D skelet door de relatieve rotaties tussen alle paren van joints. Deze 3D rotaties zijn lid van de Lie group $SO_3$, dus de representatie is een punt in de Lie groep $SO_3 \times ... \times SO_3$. Door de relative 3D rotaties te gebruiken is de representatie schaalinvariant en halveert de feature dimensie, vergeleken met \cite{Vemulapalli2014}.
			\item Een actie is dan een curve in de Lie groep $SO_3 \times ... \times SO_3$. Ze berekenen een verwachte curve, en gebruiken \gls{ac:dtw} om elke andere curve te warpen naar die curve. De \gls{ac:dtw} berekeningen zijn gebaseerd op de kwadratische euclidische afstand in de Lie algebra. Ze hebben het ook geprobeerd met geodisic distance maar da macheert niet. 
			\item Het uitrollen van de Lie group $SO_3 \times ... \times SO_3$ over zijn Lie algebra $so_3 \times ... \times so_3$ samen met elke verwachte vurve. Al de acties worden 'unwrapped' op de Lie algebra. De rolling map voor een gegeven rolling curve kan bekomen worden (theorem 2 in paper)
			\item Elke unwrapped actiecurve wordt een feature vector door alle temporale samples samen te voegen en te classificeren volgens een one-vs-all lineaire SVM classifier.
		\end{enumerate}
		\item \textbf{Gebruikte datasets:}
		\begin{itemize}
			\item FLorence3D dataset
			\item MSRPairs dataset
			\item G3D dataset
		\end{itemize}
		\item \textbf{Toekomst:}
		\begin{itemize}
			\item Rolling map kan voor elke Riemannian manifold gebruikt worden. Zij willen het zeker uitproberen op andere zaken zoals Grassmann manifold en de manifold 
			\item Zij gebruiken enkel skelet features. Indien deze gecombineerd kunnen worden met dieptebeelden zou de accuracy verhoogd kunnen worden.
			\item Zij gebruiken ook de kwadratische afstand. Het zou kunnen geoptimaliseerd worden door 'transported square root vector field' \cite{Su2014} te gebruiken aangezien deze invariant is voor temporale warping.
		\end{itemize}
	\end{itemize}
	\item Bron \cite{Slama2015}
	\begin{itemize}
		\item \textbf{Inleiding:} 
		\item \textbf{Methode:} 
		\begin{enumerate}
			\item De 3D joints worden geëxtraheerd als een tijdreeks. Via het autoregressive and moving average model wordt de dynamiek gemodelleerd. De subspace over de kolommen van de observability matrix is een punt op een grassmann manifold.
			\item Via de 'Riemannian geometry' van dit manifold kan het classificatieprobleem opgelost worden door Control Tangent ruimten te leren die het gemiddelde van elke klasse modelleren.
			
			\item Elke observatie wordt geprojecteerd op alle CTs om een Local Tangent Bandle representatie op te bouwen. Dit dient als input voor een SVM classifier.
		\end{enumerate}
		\item \textbf{Gebruikte datasets:}
		\begin{itemize}
			\item MSRAction3D 
			\item UTKinect
			\item UCFKinect
		\end{itemize}
		\item \textbf{Toekomst:}
		\begin{itemize}
			\item Uitbreiden zodat 'human behaviour recognition' mogelijk is
			\item Extra features gebruiken zodat mens-object interactie kan herkent worden
		\end{itemize}
	\end{itemize}
	\item Bron \cite{Chen2016}
	\begin{itemize}
		\item \textbf{Inleiding:} 
		\item \textbf{Methode:} 
		\begin{enumerate}
			\item
		\end{enumerate}
		\item \textbf{Gebruikte datasets:}
		\begin{itemize}
			\item MSRAction3D dataset
		\end{itemize}
		\item \textbf{Toekomst:}
		\begin{itemize}
			\item /
		\end{itemize}
	\end{itemize}
	\item Bron \cite{Chen2017}
	\begin{itemize}
		\item \textbf{Inleiding:} Een nieuwe low-cost descriptor: 3D histograms of texture (3DHoTs) die features uit dieptebeelden haalt. Dit histogram wordt opgebouwd door de dieptebeelden te projecteren op drie orthogonale cartesische vlakken
		\item \textbf{Methode:} 
		\begin{enumerate}
			\item
		\end{enumerate}
		\item \textbf{Gebruikte datasets:}
		\begin{itemize}
			\item MSRAction3D dataset
			\item MSRGesture3D dataset
			\item UTD-MHAD dataset
			\item MSRActivity3D
		\end{itemize}
		\item \textbf{Toekomst:}
		\begin{itemize}
			\item Uitbreiden zodat het ook werkt voor object herkenning en het opzoeken van images
		\end{itemize}
	\end{itemize}
	\item Bron \cite{Fanello2017}
	\begin{itemize}
		\item \textbf{Inleiding:} Ze beweren dat sparse features noodzakelijk zijn om real-time actieherkenning uit te voeren. Ze gebruiken 'one-shot learning'. Sparse coding is het benaderen van een input als een lineaire combinatie van de componenten, geselecteerd uit een lijst van basic elementen.
		\item \textbf{Methode:} 
		\begin{enumerate}
			\item Eerst berekenen ze de Region of Interest, dit is gwn de 'silhouette' van de persoon.
			\item Elke ROI binnen een frame wordt gemapped in een feature space door combinatie van 3D Histogram of Flow (3DHOF) en Global Histogram of Oriented Gradient (GHOG) op die dieptemap. 
			\item Classificatie met lineaire SVM op frame buffers.
		\end{enumerate}
		\item \textbf{Gebruikte datasets:}
		\begin{itemize}
			\item 
		\end{itemize}
		\item \textbf{Toekomst:}
		\begin{itemize}
			\item
		\end{itemize}
	\end{itemize}
	
	\item Bron \cite{Yuan2016}
	\begin{itemize}
		\item \textbf{Inleiding:} De grootste moeilijkheid bij actie localisatie is de onzekerheid van actieuitvoering en het gebruik van informatie op verschillende schalen. Zij stellen twee innovaties voor: een Pyramid of Score Distribution Feature (PSDF) om de informatie op meerdere resoluties te bewaren, gecentreerd voor een bepaalde frame. Deze PSDF wordt gecombineerd met recurrent neural networks. 
		\item \textbf{Methode:}
		\begin{enumerate}
			\item Uit een video input worden er Fisher Encoded \gls{ac:idt} features gehaald.
			\item Deze features worden gecompresseerd via \gls{ac:pca} en worden geclustered via \gls{ac:gmm}.
	
		 	Deze clusters zijn een tuple
		 	
		 	$$\{ (\pi_k, \mathbf{\mu}_k, \mathbf{\Sigma}_k) | k = 1, 2, ... K\}$$
		\end{enumerate}
		\item \textbf{Gebruikte datasets:}
		\begin{itemize}
			\item THUMOS'15
			\item MPII Cooking Activites Dataset
		\end{itemize}
	
		\item \textbf{Toekomst:}
		\begin{itemize}
			\item {\color{red}misschien dynamic size bedenken op basis van de 'motion' van frames: trage motion = grotere size}
		\end{itemize}
		\begin{itemize}
			\item 
		\end{itemize}
	\end{itemize}
\end{itemize}




\begin{itemize}
	\item Bron \cite{Wang2014}
	\begin{itemize}
		\item Actieherkenning en actiedetectie systeem voor temporally untrimmed videos door de combinatie van motion en appearance features.
		\begin{itemize}
			\item Motion feature = Fisher vector representatie met dense trajectories
			\item Appearance feature: deep convolutional neural network
		\end{itemize}
	\end{itemize}
	\item Bron \cite{Kang2016}
	\begin{itemize}
		\item Actieherkenning = het herkennen van een actie binnen een goed gedefinieerde omgeving 
		\item Actiedetectie = het herkennen en lokaliseren van acties(begin, duratie en einde) in de ruimte en de tijd
		\item training set = wordt gebruikt om classifier te trainen
		\item validation set = optioneel, bevat andere data dan de training set om de classifier te optimaliseren
		\item testing set = testen van de classifier (performance)
		\item Drie manieren om dataset op te splitsen in deze drie sets:
		\begin{itemize}
			\item voorgedefinieerde split: De dataset wordt opgesplitst  in twee of drie delen zoals de auteurs van die dataset dat vermelden
			\item n-voudige cross-validatie: Verdeeld de dataset in n gelijkvoudige stukken. Hierbij worden er (n-1)/n  percentage van de videos gebruikt om te trainen, en dan de overige 1/n om te testen. Dit proces wordt n keer herhaald, zodat elke video éénmaal gebruikt werd voor te testen
			\item leave-one-out cross-validatie: {\color{green}aangewezen methode indien testdata personen zijn.} 1 persoon wordt als testset beschouwd. De overige $n - 1$ personen is de training set.
		\end{itemize}
		
		
		\item  om actieklasse te bepalen = features extraheren en in classifier steken $\rightarrow$ classifier bepaalt actieklasse
		\item Temporally untrimmed video = delen van de video bevatten GEEN ENKELE actie. Variaties van dezelfde actie kan op hetzelfde moment voorkomen
		
		\item THUMOS challenge:
		\item 2015 $\rightarrow$ slechts één team heeft detection challenge geprobeerd
		
		\item 	Classificatietaak: de lijst van acties geven die in een lange, niet getrimde video voorkomen
		\item 	Detectietaak: ook de lijst van acties geven PLUS de plaats in tijd waar ze voorkomen
	\end{itemize}

	\item Bron \cite{Shotton2011}
	gaat eerder over hoe het skelet bepaalt wordt
	\begin{itemize}
		\item voorstel van een methode om op een accurate manier de 3D posities van de joints te bepalen, vanuit slechts één dieptebeeld, zonder temporale informatie
		\item Het bepalen van lichaamsdelen is invariant van pose, lichaamsbouw, kleren, etc...
		\item Kan runnen aan 200 fps
		\item Wordt effectief gebruikt in de Kinect software (onderzoeksteam is van Microsoft)
		\item Een dieptebeeld wordt gesegmenteerd in verschillende lichaamsdelen, aangegeven door een kleur, op basis van een kansfunctie; Elke pixel van het lichaam wordt apart behandeld en gekleurd. Een verzameling van dezelfde kleuren wordt een joint
		\item Aangezien tijdsaspect weg is, is er enkel interesse in de statische poses van een frame. Verschillen van pose in twee opeenvolgende frames is miniscuul zodat die genegeerd worden
	\end{itemize}


	\item Bron \cite{Li2010} (pre-kinect era)
	\begin{itemize}
		\item Actieherkenning met behulp van reeksen van dieptebeelden
		\item Gaan ervan uit dat efficiënte tracking van skeletbeelden nog niet mogelijk is. (is gepubliceerd zelfde jaar dat Kinect beschikbaar was, 2010)
		\item Hun oplossing is dus niet gebaseerd op het tracken van de skeletbeelden
	\end{itemize}

	\item Bron \cite{Zhao2017}
	\begin{itemize}
		\item Probleem: output van de actiecategorie EN de start en eind tijd van de actie. 
		\item Ze beweren dat actieherkenning reeds goed opgelost is, maar niet actiedetectie. Hun definities zijn: 
		\begin{itemize}
			\item Actieherkenning: De effectieve actieherkenning indien het systeem weet wanneer hij moet herkennen
			\item Actiedetectie: een langdurige video, waarbij de start en stop van een actie niet gedefinieerd zijn = untrimmed video ( videos waarbij er meerdere acties op hetzelfde moment kunnen voorkomen, alsook een irrelevante achtergrond). {\color{green} sluit heel goed aan op onze masterproef}
		\end{itemize}
		\item Uitdaging in bestaande oplossingen: groot aantal onvolledige actiefragmenten. Voorbeelden:
		\begin{itemize}
			\item Bron \cite{Singh2016}:
			\begin{itemize}
				\item maakt gebruik van \textbf{untrimmed classificatie}: de top $k = 3$ (bepaalt via cross-validation) labels worden voorspelt door globale video-level features. Daarna worden frame-level binaire classifiers gecombineerd met dynamisch programmeren om de activity proposals (die getrimmed zijn) te genereren. Elke proposal krijgt een label, gebaseerd op de globale label.
			\end{itemize}
				\item Bron \cite{Yuan2016}:
			\begin{itemize}
				\item Spreekt over de onzekerheid van het voorkomen van een actie en de moeilijkheid van het gebruik van de continue informatie

				\item Pyramid of Score Distribution Feature (PSDF) om informatie op meerdere resoluties op te vangen
				\item PSDF in combinatie met Recurrent Neural networks bieden performantiewinst in untrimmed videos.
				\item Onbekende parameters: actielabel, actieuitvoering, actiepositie, actielengte
				\item Oplossing? Per frame een verzameling van actielabels toekennen, gebruik makend van huidige frame actie-informatie en inter-frame consistentie = PSDF
	
			\end{itemize}
		
		\end{itemize}
		\item De moeilijkheid is: start, einde en duur van de actie te bepalen.
		\item Hun oplossing is \textbf{Structured Segment Network}:
		\begin{itemize}
			\item input: video
			\item output: actiecategorieën en de tijd wanneer deze voorkomen
			\item Drie stappen:
			\begin{enumerate}
				\item Een "proposal method",  om een verzameling van "temporal proposals", elk met een variërende duur en hun eigen start en eind tijd.  Elke proposal heeft drie stages: \textit{starting, course} en \textit{ending}. 
				\item Voor elke proposal wordt er STPP (structured temporal pyramid pooling) toegepast door (1) de proposal op te splitsen in drie delen; (2) temporal pyramidal representaties te maken voor elk deel; (3) een globale representatie maken voor de hele proposal.
				\item Twee classifiers worden gebruikt: herkennen van de actie en de "volledigheid" van de actie nagaan.
			\end{enumerate}
		\end{itemize}
	\end{itemize}

	\item Bron \cite{Huang2018}
	\begin{itemize}
		\item Temporal action detection = moet enerzijds detecteren of al dan niet een actie voorkomt, en anderzijds hoelang deze actie duurt, wat een uitdaging is bij untrimmed videos.
		\item Veel moderne aanpakken gaan als volgt te werk: eerst wordt er klasse-onafhankelijke proposals gegenereerd door 
	\end{itemize}

	\item Bron \cite{Deboeverie2016}
	\begin{itemize}
		\item Sliding window: laatste 30 frames bijhouden in buffer om hoge zekerheid van classificatie te voorzien; met majority voting de actie bepalen die het meeste voorkomt. 
		\item Classifier: random forests. Beslissingsbomen aanmaken via ID3 algoritme
	\end{itemize}

	\item Bron \cite{Suolan2017}
	\begin{itemize}
		\item Depth-based action recognition.
		\item \textit{key frames} worden geproduceerd uit skeletsequenties door gebruik te maken van de joints als \textbf{spatial-temporal interest points (STIPs)}. Deze worden gemapt in een dieptesequentie om een actie sequentie te representeren. De contour van de persoon wordt per frame bepaald. Op basis van deze contour en de tijd worden features opgehaald. Als classifier gebruiken ze een \textit{extreme learning machine}
		\item Voordeel van key frames: ze bevatten de meest informatieve frames. Twee methodieken om de key frames op te halen:
		\begin{enumerate}
			\item \textbf{Interframe difference}: een nieuwe key-frame wordt gekozen als het verschil tussen twee frames een bepaald threshold overschrijft.
			\item \textbf{Clustering}: groeperen van frames die op elkaar lijken op basis van low-level features. Uit die groep wordt dan de keyframe genomen, die het dichtst bij het centrum van dat cluster ligt.
		\end{enumerate}
		\item Zij gebruiken het 'opgenomen verschil': Een positie van een joint $P_{i,j}$ met $i$ het frame index en $j$ de joint index, kan gelijkgesteld worden als  $P_{i, j} = {x_{i, j}, y_{i, j}, z_{i, j}}$
		
		Het opgenomen verschil is dan:
		
		$$D_i = \sum_{j = 1}^{n} || P_{i, j} - P_{i - 1, j}||^2$$
		met $||\cdot||$ de euclidische afstand en $n$ het aantal joints.
		
		\item key frames worden dan gekozen op basis van maximum of minimum $D_i$ binnen een sliding window. Een probleem: $D_i$ is vrij laag voor de eerste en laatste aantal frames. De key frames worden dus eerder gecentraliseerd en kan de sequentie niet accuraat bepaalt worden. Stapsgewijze oplossing:
		\begin{enumerate}
			\item Voor een video met $N$ frames: neem de som van $D_i$ van $i = 2$ tot $i = N$:
			$$D_N = \sum_{i = 2}^{N}D_i$$
			\item Bepaal een aantal key frames $K$ en bereken het gemiddelde van incrementen:
			
			$$D_{avg} = D_N / K$$
			
			\item Voor $i = 2$ tot $i = L$ wordt het verschil berekent:
			
			$$W_L = D_L - k * D_{avg}, k \in K$$
			
			zodat er een verzameling ${W_L}$ is. Het minimum van deze set wordt de key frame.
		\end{enumerate}
		\item Features op basis van contour
		\item Actieherkenning met neurale netwerken (EXTREME LEARNING)
		\item \textbf{Samenvatting:}
		\begin{itemize}
			\item Actionherkenningsmethode voor kinect.
			\item Features op basis van menselijke contour van een keyframe uit een dieptebeeld. Als constraint is er het temporaal verschil.
			\item 'multi-hidden layer extreme learning machine' voor classificatie
		
		\end{itemize}
		
	\end{itemize}

	\item Bron \cite{Carlsson2001}
	\begin{itemize}
		\item Een bepaalde actie kan onderverdeeld worden in een sequentie van poses.
	\end{itemize}	

	\item Bron \cite{Vezzani2010}
	\begin{itemize}
		\item HMM laten toe om de temporale evolutie te modelleren.
		\item De \textbf{feature set} en \textbf{emission probability function} moeten goed gedefinieerd zijn. 
		\item Gegeven een set $C$ van actie classes $\Lambda = \lambda^1 ... \lambda^C$, zoek de klasse $\lambda^*$ die de kans op $P(\lambda|O)$ maximaliseert met $O = \{o_1 ... o_T\}$ de frame observaties.
		\item Een HMM voor elke actie. Classificatie voor een observatiereeks $O$ wordt uitgevoerd door het model te pakken met de hoogste waarschijnlijkheid:
		
				
		$$\lambda^* = \arg \max_{1\leq c\leq C} \big[P(O|\lambda^c)\big]$$
		\item Twee verschillende features gebruikt:
		\begin{itemize}
			\item Projection histograms
			\item Shape descriptors
		\end{itemize}
	\end{itemize}
\end{itemize}




\section{Machine learning}
\subsection{Features}
\begin{itemize}
	\item Een \textbf{feature} is een individueel, meetbare eigenschap of karakteristiek van een object dat geobserveerd wordt.
	\item Eigenschappen:
	\begin{itemize}
		\item Informatief: de informatiewinst van de feature moet hoog zijn
		\item Discriminative: op basis van de feature moet het eenvoudig zijn om het onderscheid te maken tussen de verschillende klassen
		\item Onafhankelijk: De feature op zich mag van geen andere feature of meetwaarde van dezelfde feature afhangen.


	\end{itemize}
	\item Een \textbf{sparse feature discriptor} heeft een variabel aantal features. Een \textbf{dense feature descriptor} heeft een vast aantal features.
	\item \textbf{Feature extraction} ($\equiv$ dimensionality reduction) is het verzamelen van features uit ruwe data zodat deze kunnen gebruikt worden als feature vector bij een classifier. 
	\item Een \textbf{feature vector} is een $n$-dimensionale vector van \underline{numerieke} features.
	\item De \textbf{feature space} ($\equiv$ vectorruimte) beschrijft de ruimte waarin de features zich bevinden. (bv 3 verschillende features = $\mathcal{R}^3$)
	\item \textbf{Feature construction} is het maken van nieuwe features op basis van reeds bestaande features. De mapping is een functie $\phi$, van $\mathcal{R}^n$ naar $\mathcal{R}^{n + 1}$, met $f$ de geconstrueerde feature op basis van bestaande features, bv $f = x_1/x_2$.
	
	$$\phi(x_1, x_2, ..., x_n) = (x_1, x_2, ... x_n, f)$$

\end{itemize}
\subsection{Classifier}
\begin{itemize}
	\item Identificeren tot welke klasse een \underline{nieuwe} observatie behoort, gebaseerd op een training set waarvan de klassen wel gekend zijn.
	\item \textbf{Lineaire classifiers} geven aan elke klasse $k$ een score op basis van de combinatie van de feature vector met een gewichtenvector met het scalair product.  De gekozen klasse is dan die met de hoogste score. Eenvoudiger geschreven:	
	$$
	score(X_i, k) = \beta_k \cdot X_i
	$$
	\begin{itemize}
		\item $X_i$ = de feature vector voor instantie $i$
		\item $\beta_k$ = de gewichtenvector voor klasse $k$
	\end{itemize}
	\item \todo{later onderzoeken}
	\item \textbf{Support Vector machines}
	\item \textbf{Random forests}
	\item \textbf{Boosting}
\end{itemize}




\subsection{Hidden Markov Model}
\label{sec:hidden_markov_model}
Bron \cite{Ramage2007}
\subsubsection{Markov model}

\begin{itemize}
	\item Markov process = stochastisch process met volgende eigenschappen:
	\begin{itemize}
		\item Het aantal toestanden $S = \{s_1, s_2, ... s_{|S|}\}$ is eindig.
		\item Observatie van een sequentie over de tijd $\textbf{z} \in S^T$
		
		Voorbeeld: 
		
		$S = \{sun, cloud, rain\}$ met $|S| = 3$ en $\textbf{z} = \{z_1=S_{sun},z_2=S_{cloud},z_3=S_{cloud},z_4=S_{rain},z_5=S_{cloud}\}$ met $T = 5$.
		\item Markov Assumpties:
		\begin{enumerate}
			\item Een toestand is enkel afhankelijk van de vorige toestand (\textbf{Markov Property}).
			
			$$P(z_t | z_{t - 1},z_{t-2}, ..., z_1) = P(z_t|z_{t-1})$$
			\item De waarschijnlijk is constant in de tijd
			
			$$P(z_t|z_{t - 1}) = P(z_2|z_1); t \in 2 ...T$$
		\end{enumerate}
		\item Beginstaat $z_0 \equiv s_0$.
		\item Transitiematrix $A \in \mathbb{R}^{(|S| + 1)\times(|S| + 1)}$, met $A_{ij} = P(i \rightarrow j)$, :
		
		$$A = \begin{matrix}
		     & s_0 & s_{sun} & s_{cloud} & s_{rain} \\
		 s_0 & 0 & .33 & .33 & .33 \\
		 s_{sun} & 0 & .8 & .1 & .1 \\
		 s_{cloud} & 0 & .2 & .6 & .2 \\
		 s_{rain} & 0 & .1 & .2 & .7 \\
		\end{matrix}$$
	\end{itemize}

\end{itemize}

\begin{itemize}
	\item Twee vragen die een markov model kunnen oplossen:
	\begin{enumerate}
		\item Wat is de kans op een bepaalde toestandenvector $\textbf{z}$?

				$$	P(\textbf{z})  = \prod_{t=1}^{T} A_{z_{t-1}z_t}$$

			Stel $\textbf{z} = \{z_1=S_{sun},z_2=S_{cloud},z_3=S_{rain},z_4=S_{rain},z_5=S_{cloud}\}$ dan is $P(\textbf{z}) = 0.33 \times 0.1 \times 0.2 \times 0.7 \times 0.2 $
		\item Gegeven $\textbf{z}$, hoe worden best de parameters van $A$ benaderd zodat de kans op $\textbf{z}$ maximaal is.
		$$A_{ij} = \frac{\sum_{t=1}^{T} \{z_{t-1} = s_i \wedge z_t = s_j\}}{\sum_{t=1}^{T} \{z_{t-1} = s_i\}}$$
		
		De maximale kans om van staat $i$ naar $j$ te gaan is het aantal transities van $i$ naar $j$ gedeeld door het totaal aantal keer dat we in $i$ zitten. Met andere woorden: Hoeveel \% zaten we in $i$ als we van $j$ komen.
	\end{enumerate}
\end{itemize}

\subsubsection{Hidden Markov Model}
\begin{itemize}
	\item \gls{ac:hmm} = Veronderstelt een markov process met verborgen toestanden
	\item Bij een HMM: de staat is niet zichtbaar, maar de output is wel zichtbaar.
	\item Formeel:
	\begin{itemize}
		\item Sequentie van geobserveerde outputs $x = \{x_1, x_2, ..., x_T\}$  uit een alfabet $V = \{v_1, v_2, ..., v_{|V|}\}$
		\item Er is ook een sequentie van staten $z =\{z_1, z_2, ... z_T\}$ uit een alfabet $S = \{s_1, s_2, ... s_{|S|}\}$, maar deze zijn \underline{niet zichtbaar}.
		\item Transitiematrix $A_{ij}$ wel bekend.
		\item Kans dat een bepaalde output gegenereerd wordt in functie van de verborgen toestanden:
		
		$$P(x_t = v_k|z_t = s_j) = P(x_t = v_k | x_1, ..., x_T, z_1, ..., z_T) = B_{jk}$$
		
		Matrix $B$ geeft de waarschijnlijkheid dat een verborgen toestand $s_j$ de output $v_k$ teruggeeft.
	\end{itemize}
	\item Vaak voorkomende problemen die opgelost kunnen worden met HMM:
	\begin{itemize}
		\item Gegeven de parameters en geobserveerde data, benader de optimale sequentie van verborgen toestanden.
		\item Gegeven de parameters en geobserveerde data, bereken de kans op die data. $\rightarrow$ Wordt het 'decoding' probleem (\textbf{Viterbi Algoritme}) genoemd en wordt gebruikt bij continue actieherkenning.
		\item Gegeven de geobserveerde data, benader de parameters van $A$ en $B$.
	\end{itemize}
	\item Het \textbf{decoding probleem}: \url{http://jedlik.phy.bme.hu/~gerjanos/HMM/node8.html}
	\begin{itemize}
		\item Zoek de meest waarschijnlijke reeks van toestanden $\textbf{z} \in S^T$ voor een verzameling van observaties $\textbf{x} \in V^T$.
		
		\item Hoe 'meest waarschijnlijke toestandensequentie' definiëren. Een mogelijke manier is om de meest waarschijnlijke staat $s_t$ voor $x_t$ te berekenen, en alle $q_t$ dier daar aan voldoen te concateneren.
		Andere manier is \textbf{Viterbi algoritme} die de hele toestandensequentie met de grootste waarschijnlijkheid teruggeeft.
		
		\item Hulpvariabele:
		
		$$\delta_t(i) = \max_{\substack{q_1, q_2, ... q_{t-1}}} p\{q_1, q_2, ... q_{t - 1}, q_t = i, o_1, o_2, ... o_{t - 1} | \lambda \}$$
		
		die de hoogste kans beschrijft dat een partiele observatie en toestandensequentie tot $t = t$ kan hebben, wanneer de huidige staat $i$ is.
	\end{itemize}
\end{itemize}


